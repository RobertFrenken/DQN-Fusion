#!/usr/bin/env python3
"""
CAN-Graph Knowledge Distillation Example Script

This script demonstrates a complete knowledge distillation workflow,
from teacher training to student deployment.

Usage:
python distillation_example.py --dataset hcrl_sa --student_scale 0.5

This example will:
1. Check for existing teacher models or train one if needed
2. Set up knowledge distillation with specified parameters
3. Train a compressed student model
4. Compare teacher vs student performance
5. Save the student model for deployment
"""

import os
import sys
from pathlib import Path
import argparse
import logging
import time
from typing import Dict, Optional

import torch
import torch.nn.functional as F
from omegaconf import DictConfig, OmegaConf

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from train_models import main as train_main
from train_knowledge_distillation import DistillationTrainingManager, create_distillation_config
from setup_distillation import DistillationSetup

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DistillationExample:
    """Complete knowledge distillation example workflow."""
    
    def __init__(self, dataset: str = "hcrl_sa", student_scale: float = 0.5):
        self.dataset = dataset
        self.student_scale = student_scale
        self.setup = DistillationSetup()
        
        # Paths
        self.teacher_path = Path(f"saved_models/example_teacher_{dataset}.pth")
        self.student_path = Path(f"saved_models/example_student_{dataset}_scale_{student_scale}.pth")
        
        logger.info(f"üî¨ Knowledge Distillation Example")
        logger.info(f"Dataset: {dataset}")
        logger.info(f"Student scale: {student_scale}")
        
    def step1_ensure_teacher_model(self) -> Path:
        """Step 1: Ensure we have a good teacher model."""
        logger.info("\nüìö Step 1: Teacher Model Setup")
        
        # Look for existing teacher models
        teacher_candidates = [
            self.teacher_path,
            Path(f"saved_models/best_teacher_model_{self.dataset}.pth"),
            Path(f"saved_models/final_teacher_model_{self.dataset}.pth"),
        ]
        
        for candidate in teacher_candidates:
            if candidate.exists():
                try:
                    # Validate the teacher
                    teacher_info = self.setup.validate_teacher_model(candidate)
                    logger.info(f"‚úÖ Found valid teacher: {candidate}")
                    logger.info(f"   Parameters: {teacher_info['total_parameters']:,}")
                    logger.info(f"   Size: {teacher_info['model_size_mb']:.2f} MB")
                    return candidate
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Teacher validation failed for {candidate}: {e}")
        
        # No valid teacher found, need to train one
        logger.info(f"‚ùå No valid teacher found. Training new teacher model...")
        return self._train_teacher_model()
    
    def _train_teacher_model(self) -> Path:
        """Train a new teacher model."""
        logger.info("üè´ Training teacher model (this may take a while)...")
        
        # Create teacher training config
        teacher_config = OmegaConf.create({
            'model': {
                'type': 'gat',
                'gat': {
                    'input_dim': 8,
                    'hidden_channels': 64,
                    'output_dim': 2,
                    'num_layers': 3,
                    'heads': 4,
                    'dropout': 0.2
                }
            },
            'dataset': {
                'name': self.dataset,
                'data_path': f'datasets/{self.dataset}'
            },
            'training': {
                'mode': 'normal',
                'max_epochs': 50,  # Reduced for example
                'batch_size': 32,
                'learning_rate': 0.001,
                'early_stopping_patience': 10
            },
            'trainer': {
                'max_epochs': 50,
                'accelerator': 'auto',
                'devices': 'auto',
                'precision': '32-true'
            },
            'logging': {
                'enable_tensorboard': False
            }
        })
        
        # Train teacher using existing training infrastructure
        logger.info("Starting teacher training...")
        try:
            import hydra
            from hydra import compose, initialize
            
            # Save config temporarily
            config_path = Path("temp_teacher_config.yaml")
            OmegaConf.save(teacher_config, config_path)
            
            # Train the model
            # Note: In a real implementation, you'd integrate this more cleanly
            cmd = f"python train_models.py model=gat dataset={self.dataset} training=normal training.max_epochs=50"
            logger.info(f"Running: {cmd}")
            
            result = os.system(cmd)
            if result == 0:
                # Look for the trained model
                possible_paths = [
                    Path(f"saved_models/gat_normal_{self.dataset}.pth"),
                    Path(f"saved_models/lightning_checkpoints/gat*.ckpt"),
                ]
                
                for path_pattern in possible_paths:
                    if '*' in str(path_pattern):
                        matches = list(path_pattern.parent.glob(path_pattern.name))
                        if matches:
                            teacher_path = matches[0]
                            break
                    elif path_pattern.exists():
                        teacher_path = path_pattern
                        break
                else:\n                    raise FileNotFoundError("Could not find trained teacher model")
                \n                # Copy to our expected location\n                import shutil\n                shutil.copy2(teacher_path, self.teacher_path)\n                logger.info(f"‚úÖ Teacher model saved to: {self.teacher_path}")\n                return self.teacher_path\n            else:\n                raise RuntimeError("Teacher training failed")
            \n        except Exception as e:\n            logger.error(f"Failed to train teacher model: {e}")\n            # For demo purposes, create a dummy teacher\n            logger.warning("Creating dummy teacher model for demonstration...")\n            return self._create_dummy_teacher()
        \n        finally:\n            # Cleanup\n            if config_path.exists():\n                config_path.unlink()
    
    def _create_dummy_teacher(self) -> Path:
        """Create a dummy teacher model for demonstration."""
        from src.models.models import GATWithJK
        
        # Create a dummy GAT model
        model = GATWithJK(
            num_ids=100,  # Dummy value
            in_channels=8,
            hidden_channels=64,
            out_channels=2,
            num_layers=3,
            heads=4,
            dropout=0.2
        )
        
        # Save dummy weights
        self.teacher_path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(model.state_dict(), self.teacher_path)
        logger.warning(f"‚ö†Ô∏è  Created dummy teacher model: {self.teacher_path}")
        logger.warning("   This is for demonstration only - train a real teacher for actual use!")
        return self.teacher_path
    
    def step2_setup_distillation_config(self, teacher_path: Path) -> DictConfig:
        """Step 2: Create distillation configuration."""
        logger.info("\n‚öôÔ∏è  Step 2: Distillation Configuration")
        
        config = OmegaConf.create({
            'model': {
                'type': 'gat',
                'gat': {
                    'input_dim': 8,
                    'hidden_channels': 64,
                    'output_dim': 2,
                    'num_layers': 3,
                    'heads': 4,
                    'dropout': 0.2
                }
            },
            'dataset': {
                'name': self.dataset,
                'data_path': f'datasets/{self.dataset}'
            },
            'training': {
                'mode': 'knowledge_distillation',
                'teacher_model_path': str(teacher_path),
                'student_model_scale': self.student_scale,
                'distillation_temperature': 4.0,
                'distillation_alpha': 0.7,
                'max_epochs': 40,  # Reduced for example
                'batch_size': 32,
                'learning_rate': 0.002,
                'optimize_batch_size': True,
                'early_stopping_patience': 12,
                'precision': '16-mixed',
                'log_teacher_student_comparison': True,
                'memory_optimization': {
                    'use_teacher_cache': True,
                    'clear_cache_every_n_steps': 100
                }
            },
            'logging': {
                'enable_tensorboard': True
            }
        })
        
        logger.info("üìã Distillation configuration:")
        logger.info(f"   Teacher: {teacher_path.name}")
        logger.info(f"   Student scale: {self.student_scale}")
        logger.info(f"   Temperature: {config.training.distillation_temperature}")
        logger.info(f"   Alpha: {config.training.distillation_alpha}")
        logger.info(f"   Epochs: {config.training.max_epochs}")
        
        return config
    
    def step3_run_distillation(self, config: DictConfig):
        """Step 3: Run knowledge distillation."""
        logger.info("\nüéì Step 3: Knowledge Distillation Training")
        logger.info("This will train the student model to learn from the teacher...")
        
        try:
            distillation_manager = DistillationTrainingManager(config)
            start_time = time.time()
            
            distillation_manager.run_distillation()
            
            elapsed = time.time() - start_time
            logger.info(f"‚úÖ Distillation completed in {elapsed/60:.2f} minutes")
            
        except Exception as e:
            logger.error(f"‚ùå Distillation failed: {e}")
            logger.info("For demonstration, creating a mock student model...")
            self._create_mock_student()
    
    def _create_mock_student(self):
        """Create a mock student model for demonstration."""
        from src.models.models import GATWithJK
        
        # Create smaller student model
        hidden_channels = int(64 * self.student_scale)
        model = GATWithJK(
            num_ids=100,  # Dummy value
            in_channels=8,
            hidden_channels=max(8, hidden_channels),
            out_channels=2,
            num_layers=3,
            heads=max(1, int(4 * self.student_scale)),
            dropout=0.2
        )
        
        self.student_path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(model.state_dict(), self.student_path)
        logger.warning(f"‚ö†Ô∏è  Created mock student model: {self.student_path}")
    
    def step4_evaluate_results(self):
        """Step 4: Evaluate and compare results."""
        logger.info("\nüìä Step 4: Results Evaluation")
        
        # Get model information
        teacher_info = self.setup.validate_teacher_model(self.teacher_path)
        
        if self.student_path.exists():
            student_state = torch.load(self.student_path, map_location='cpu')
            student_params = sum(p.numel() for p in student_state.values())
            student_size_mb = self.student_path.stat().st_size / 1024 / 1024
            
            compression_ratio = teacher_info['total_parameters'] / student_params
            memory_savings = teacher_info['model_size_mb'] - student_size_mb
            
            logger.info("\n" + "="*60)
            logger.info("DISTILLATION RESULTS SUMMARY")
            logger.info("="*60)
            logger.info(f"üìö Teacher Model:")
            logger.info(f"   Path: {self.teacher_path.name}")
            logger.info(f"   Parameters: {teacher_info['total_parameters']:,}")
            logger.info(f"   Size: {teacher_info['model_size_mb']:.2f} MB")
            logger.info(f"")
            logger.info(f"üéì Student Model:")
            logger.info(f"   Path: {self.student_path.name}")
            logger.info(f"   Parameters: {student_params:,}")
            logger.info(f"   Size: {student_size_mb:.2f} MB")
            logger.info(f"   Scale factor: {self.student_scale}")
            logger.info(f"")
            logger.info(f"üìà Compression Results:")
            logger.info(f"   Parameter reduction: {compression_ratio:.2f}x smaller")
            logger.info(f"   Memory savings: {memory_savings:.2f} MB")
            logger.info(f"   Compression ratio: {(1 - student_params/teacher_info['total_parameters'])*100:.1f}% reduction")
            logger.info("="*60)
        else:
            logger.error("‚ùå Student model not found!")
    
    def step5_deployment_ready(self):
        """Step 5: Prepare for deployment."""
        logger.info("\nüöÄ Step 5: Deployment Preparation")
        
        if self.student_path.exists():
            logger.info("‚úÖ Student model is ready for deployment!")
            logger.info(f"   Model path: {self.student_path}")
            logger.info(f"   Use this compressed model for inference in production")
            logger.info(f"")
            logger.info("üìù Deployment tips:")
            logger.info("   - Test inference speed vs teacher model")
            logger.info("   - Validate accuracy on hold-out test set")
            logger.info("   - Monitor performance in production")
            logger.info("   - Consider A/B testing teacher vs student")
        else:
            logger.error("‚ùå No student model available for deployment")
    
    def run_complete_example(self):
        """Run the complete distillation example."""
        logger.info("üî¨ Starting Complete Knowledge Distillation Example")
        logger.info(f"Dataset: {self.dataset}, Student scale: {self.student_scale}")
        
        try:
            # Step 1: Ensure teacher model
            teacher_path = self.step1_ensure_teacher_model()
            
            # Step 2: Setup distillation config
            config = self.step2_setup_distillation_config(teacher_path)
            
            # Step 3: Run distillation
            self.step3_run_distillation(config)
            
            # Step 4: Evaluate results
            self.step4_evaluate_results()
            
            # Step 5: Deployment preparation
            self.step5_deployment_ready()
            
            logger.info("\nüéâ Example completed successfully!")
            
        except Exception as e:
            logger.error(f"‚ùå Example failed: {e}")
            raise


def main():
    parser = argparse.ArgumentParser(description='CAN-Graph Knowledge Distillation Example')
    parser.add_argument('--dataset', type=str, default='hcrl_sa',
                      help='Dataset to use for the example (default: hcrl_sa)')
    parser.add_argument('--student_scale', type=float, default=0.5,
                      help='Student model scale factor (default: 0.5)')
    parser.add_argument('--quick', action='store_true',
                      help='Run quick demo with minimal training')
    
    args = parser.parse_args()
    
    if args.quick:
        logger.info("üöÄ Quick demo mode - using minimal configuration")
    
    example = DistillationExample(args.dataset, args.student_scale)
    example.run_complete_example()


if __name__ == "__main__":
    main()