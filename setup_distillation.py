#!/usr/bin/env python3\n\"\"\"\nKnowledge Distillation Setup and Launcher Script\n\nThis script helps you quickly set up and run knowledge distillation experiments\nwith proper teacher model validation and student model configuration.\n\nUsage examples:\n\n1. Quick start with existing teacher model:\npython setup_distillation.py --quick-setup \\\n    --teacher saved_models/best_teacher_model_hcrl_sa.pth \\\n    --dataset hcrl_sa \\\n    --student-scale 0.5\n\n2. List available teacher models:\npython setup_distillation.py --list-teachers\n\n3. Interactive setup:\npython setup_distillation.py --interactive\n\n4. Validate teacher model:\npython setup_distillation.py --validate-teacher saved_models/best_teacher_model_hcrl_sa.pth\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\nimport argparse\nimport logging\nfrom typing import List, Dict, Tuple\n\nimport torch\nfrom omegaconf import OmegaConf\n\n# Add project root to path\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\nlogging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\nlogger = logging.getLogger(__name__)\n\n\nclass DistillationSetup:\n    \"\"\"Helper class for setting up knowledge distillation experiments.\"\"\"\n    \n    def __init__(self):\n        self.project_root = Path.cwd()\n        self.saved_models_dir = self.project_root / \"saved_models\"\n        self.datasets_dir = self.project_root / \"datasets\"\n        \n    def list_available_teachers(self) -> List[Path]:\n        \"\"\"List all available teacher models.\"\"\"\n        teacher_patterns = [\n            \"*teacher*.pth\",\n            \"best_*.pth\",\n            \"final_*.pth\"\n        ]\n        \n        teacher_models = []\n        if self.saved_models_dir.exists():\n            for pattern in teacher_patterns:\n                teacher_models.extend(self.saved_models_dir.glob(pattern))\n                teacher_models.extend(self.saved_models_dir.rglob(pattern))\n        \n        return list(set(teacher_models))  # Remove duplicates\n    \n    def list_available_datasets(self) -> List[str]:\n        \"\"\"List all available datasets.\"\"\"\n        datasets = []\n        if self.datasets_dir.exists():\n            for item in self.datasets_dir.iterdir():\n                if item.is_dir() and not item.name.startswith('.'):\n                    # Check if it looks like a dataset (has some expected files)\n                    if any(item.glob(\"*.csv\")) or any(item.glob(\"train_*\")):\n                        datasets.append(item.name)\n        return datasets\n    \n    def validate_teacher_model(self, teacher_path: Path) -> Dict:\n        \"\"\"Validate teacher model and return information.\"\"\"\n        if not teacher_path.exists():\n            raise FileNotFoundError(f\"Teacher model not found: {teacher_path}\")\n        \n        try:\n            # Load model to check format and get info\n            state = torch.load(teacher_path, map_location='cpu')\n            \n            # Determine format\n            if 'state_dict' in state:\n                model_dict = state['state_dict']\n                format_type = \"Lightning checkpoint\"\n                extra_info = {\n                    'epoch': state.get('epoch', 'unknown'),\n                    'global_step': state.get('global_step', 'unknown')\n                }\n            else:\n                model_dict = state\n                format_type = \"Direct state dict\"\n                extra_info = {}\n            \n            # Count parameters\n            total_params = sum(p.numel() for p in model_dict.values())\n            model_size_mb = teacher_path.stat().st_size / 1024 / 1024\n            \n            # Analyze architecture from keys\n            gat_layers = len([k for k in model_dict.keys() if 'convs.' in k and '.weight' in k])\n            has_embedding = any('id_embedding' in k for k in model_dict.keys())\n            has_fc = any('fc_layers' in k for k in model_dict.keys())\n            \n            info = {\n                'path': teacher_path,\n                'format': format_type,\n                'file_size_mb': model_size_mb,\n                'total_parameters': total_params,\n                'estimated_memory_mb': total_params * 4 / 1024 / 1024,\n                'architecture': {\n                    'gat_layers': gat_layers,\n                    'has_id_embedding': has_embedding,\n                    'has_fc_layers': has_fc\n                },\n                'extra_info': extra_info\n            }\n            \n            return info\n            \n        except Exception as e:\n            raise RuntimeError(f\"Failed to validate teacher model: {e}\")\n    \n    def create_distillation_config(self, teacher_path: str, dataset: str, \n                                 student_scale: float = 1.0, \n                                 alpha: float = 0.7, \n                                 temperature: float = 4.0,\n                                 epochs: int = 80) -> str:\n        \"\"\"Create a distillation configuration file.\"\"\"\n        \n        config_content = f\"\"\"\n# Knowledge Distillation Configuration\n# Generated by setup_distillation.py\n\ndefaults:\n  - model: gat\n  - dataset: {dataset}\n  - training: knowledge_distillation\n  - _self_\n\n# Override training settings for distillation\ntraining:\n  mode: knowledge_distillation\n  teacher_model_path: {teacher_path}\n  student_model_scale: {student_scale}\n  distillation_alpha: {alpha}\n  distillation_temperature: {temperature}\n  max_epochs: {epochs}\n  \n  # Memory optimizations for distillation\n  batch_size: 32\n  precision: '16-mixed'\n  accumulate_grad_batches: 2\n  optimize_batch_size: true\n  \n  # Enhanced monitoring\n  log_teacher_student_comparison: true\n  early_stopping_patience: 15\n  \n  # Caching for efficiency\n  memory_optimization:\n    use_teacher_cache: true\n    clear_cache_every_n_steps: 100\n\n# Experiment tracking\nexperiment_name: \"distillation_{dataset}_scale_{student_scale}_alpha_{alpha}\"\n\n# Logging\nlogging:\n  enable_tensorboard: true\n\"\"\"\n        \n        config_path = Path(f\"conf/experiments/distillation_{dataset}_scale_{student_scale}.yaml\")\n        config_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        with open(config_path, 'w') as f:\n            f.write(config_content)\n        \n        return str(config_path)\n    \n    def interactive_setup(self):\n        \"\"\"Interactive setup for distillation experiment.\"\"\"\n        print(\"\\nüî¨ Interactive Knowledge Distillation Setup\")\n        print(\"=\" * 50)\n        \n        # List available teachers\n        teachers = self.list_available_teachers()\n        if not teachers:\n            print(\"‚ùå No teacher models found in saved_models directory!\")\n            print(\"   Train a teacher model first using normal training mode.\")\n            return\n        \n        print(\"\\nüìö Available teacher models:\")\n        for i, teacher in enumerate(teachers, 1):\n            try:\n                info = self.validate_teacher_model(teacher)\n                print(f\"  {i}. {teacher.name} ({info['file_size_mb']:.1f}MB, {info['total_parameters']:,} params)\")\n            except Exception as e:\n                print(f\"  {i}. {teacher.name} (validation failed: {str(e)[:50]}...)\")\n        \n        # Select teacher\n        while True:\n            try:\n                choice = input(f\"\\nSelect teacher model (1-{len(teachers)}): \")\n                teacher_idx = int(choice) - 1\n                if 0 <= teacher_idx < len(teachers):\n                    selected_teacher = teachers[teacher_idx]\n                    break\n                else:\n                    print(f\"Please enter a number between 1 and {len(teachers)}\")\n            except ValueError:\n                print(\"Please enter a valid number\")\n        \n        # Validate selected teacher\n        try:\n            teacher_info = self.validate_teacher_model(selected_teacher)\n            print(f\"\\n‚úÖ Teacher model validated: {selected_teacher.name}\")\n            print(f\"   Parameters: {teacher_info['total_parameters']:,}\")\n            print(f\"   Estimated GPU memory: {teacher_info['estimated_memory_mb']:.1f}MB\")\n        except Exception as e:\n            print(f\"‚ùå Teacher validation failed: {e}\")\n            return\n        \n        # List available datasets\n        datasets = self.list_available_datasets()\n        if not datasets:\n            print(\"\\n‚ùå No datasets found!\")\n            return\n        \n        print(f\"\\nüìÅ Available datasets: {', '.join(datasets)}\")\n        while True:\n            dataset = input(\"Enter dataset name: \").strip()\n            if dataset in datasets:\n                break\n            else:\n                print(f\"Dataset '{dataset}' not found. Available: {', '.join(datasets)}\")\n        \n        # Get distillation parameters\n        print(\"\\n‚öôÔ∏è Distillation parameters:\")\n        \n        student_scale = float(input(\"Student model scale (0.5 for half size, 1.0 for same size) [1.0]: \") or \"1.0\")\n        alpha = float(input(\"Distillation alpha (weight for soft targets) [0.7]: \") or \"0.7\")\n        temperature = float(input(\"Distillation temperature [4.0]: \") or \"4.0\")\n        epochs = int(input(\"Training epochs [80]: \") or \"80\")\n        \n        # Create config\n        config_path = self.create_distillation_config(\n            str(selected_teacher), dataset, student_scale, alpha, temperature, epochs\n        )\n        \n        print(f\"\\n‚úÖ Configuration created: {config_path}\")\n        \n        # Generate command\n        cmd = f\"python train_knowledge_distillation.py --config-path {config_path}\"\n        print(f\"\\nüöÄ Run distillation with:\")\n        print(f\"   {cmd}\")\n        \n        # Ask if user wants to run now\n        run_now = input(\"\\nRun training now? (y/N): \").strip().lower()\n        if run_now == 'y':\n            print(f\"\\nüèÉ Starting training...\")\n            os.system(cmd)\n    \n    def quick_setup(self, teacher_path: str, dataset: str, student_scale: float = 1.0):\n        \"\"\"Quick setup and launch distillation.\"\"\"\n        teacher_path_obj = Path(teacher_path)\n        \n        # Validate inputs\n        if not teacher_path_obj.exists():\n            print(f\"‚ùå Teacher model not found: {teacher_path}\")\n            return\n        \n        if dataset not in self.list_available_datasets():\n            print(f\"‚ùå Dataset '{dataset}' not found. Available: {', '.join(self.list_available_datasets())}\")\n            return\n        \n        # Validate teacher\n        try:\n            teacher_info = self.validate_teacher_model(teacher_path_obj)\n            print(f\"‚úÖ Teacher validated: {teacher_info['total_parameters']:,} parameters\")\n        except Exception as e:\n            print(f\"‚ùå Teacher validation failed: {e}\")\n            return\n        \n        # Run distillation directly\n        cmd = [\n            \"python\", \"train_knowledge_distillation.py\",\n            \"--teacher_path\", teacher_path,\n            \"--dataset\", dataset,\n            \"--student_scale\", str(student_scale),\n            \"--tensorboard\"\n        ]\n        \n        print(f\"\\nüöÄ Starting knowledge distillation...\")\n        print(f\"Command: {' '.join(cmd)}\")\n        os.execvp(\"python\", cmd)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Knowledge Distillation Setup and Launcher',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=__doc__\n    )\n    \n    parser.add_argument('--list-teachers', action='store_true',\n                      help='List available teacher models')\n    parser.add_argument('--list-datasets', action='store_true',\n                      help='List available datasets')\n    parser.add_argument('--validate-teacher', type=str,\n                      help='Validate a teacher model')\n    parser.add_argument('--interactive', action='store_true',\n                      help='Interactive setup mode')\n    parser.add_argument('--quick-setup', action='store_true',\n                      help='Quick setup and launch')\n    \n    # Quick setup options\n    parser.add_argument('--teacher', type=str,\n                      help='Teacher model path (for quick setup)')\n    parser.add_argument('--dataset', type=str,\n                      help='Dataset name (for quick setup)')\n    parser.add_argument('--student-scale', type=float, default=1.0,\n                      help='Student model scale factor')\n    \n    args = parser.parse_args()\n    \n    setup = DistillationSetup()\n    \n    if args.list_teachers:\n        teachers = setup.list_available_teachers()\n        if teachers:\n            print(\"\\nüìö Available teacher models:\")\n            for teacher in teachers:\n                try:\n                    info = setup.validate_teacher_model(teacher)\n                    print(f\"  - {teacher} ({info['file_size_mb']:.1f}MB, {info['total_parameters']:,} params)\")\n                except Exception as e:\n                    print(f\"  - {teacher} (validation failed)\")\n        else:\n            print(\"‚ùå No teacher models found\")\n    \n    elif args.list_datasets:\n        datasets = setup.list_available_datasets()\n        if datasets:\n            print(f\"\\nüìÅ Available datasets: {', '.join(datasets)}\")\n        else:\n            print(\"‚ùå No datasets found\")\n    \n    elif args.validate_teacher:\n        try:\n            info = setup.validate_teacher_model(Path(args.validate_teacher))\n            print(f\"\\n‚úÖ Teacher model validation successful:\")\n            print(f\"   Path: {info['path']}\")\n            print(f\"   Format: {info['format']}\")\n            print(f\"   File size: {info['file_size_mb']:.1f}MB\")\n            print(f\"   Parameters: {info['total_parameters']:,}\")\n            print(f\"   Estimated memory: {info['estimated_memory_mb']:.1f}MB\")\n            print(f\"   Architecture: {info['architecture']}\")\n            if info['extra_info']:\n                print(f\"   Extra info: {info['extra_info']}\")\n        except Exception as e:\n            print(f\"‚ùå Validation failed: {e}\")\n    \n    elif args.interactive:\n        setup.interactive_setup()\n    \n    elif args.quick_setup:\n        if not args.teacher or not args.dataset:\n            print(\"‚ùå Quick setup requires --teacher and --dataset arguments\")\n            return\n        setup.quick_setup(args.teacher, args.dataset, args.student_scale)\n    \n    else:\n        print(\"üî¨ Knowledge Distillation Setup\")\n        print(\"\\nUse --help to see available options, or --interactive for guided setup.\")\n        \n        # Show quick summary\n        teachers = setup.list_available_teachers()\n        datasets = setup.list_available_datasets()\n        print(f\"\\nFound {len(teachers)} teacher models and {len(datasets)} datasets.\")\n        \n        if teachers and datasets:\n            print(\"\\nüí° Quick start example:\")\n            print(f\"   python {Path(__file__).name} --quick-setup --teacher {teachers[0]} --dataset {datasets[0]}\")\n\n\nif __name__ == \"__main__\":\n    main()\n