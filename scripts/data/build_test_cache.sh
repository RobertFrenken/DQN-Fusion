#!/bin/bash
# Build test graph caches â€” submits one SLURM job per dataset.
# Usage: bash scripts/data/build_test_cache.sh                    # All datasets (set_01-04)
#        bash scripts/data/build_test_cache.sh set_02 set_03      # Specific datasets

set -euo pipefail

PROJECT_DIR="$(cd "$(dirname "$0")/.." && pwd)"
mkdir -p "$PROJECT_DIR/slurm_logs"

if [ $# -gt 0 ]; then
    DATASETS="$@"
else
    DATASETS="set_01 set_02 set_03 set_04"
fi

for ds in $DATASETS; do
    sbatch --account=PAS3209 --partition=cpu \
      --time=240 --mem=85G --cpus-per-task=8 \
      --job-name="test-cache-${ds}" \
      --output="$PROJECT_DIR/slurm_logs/%j-test-cache-${ds}.out" \
      --error="$PROJECT_DIR/slurm_logs/%j-test-cache-${ds}.err" \
      --wrap="source ~/KD-GAT/.venv/bin/activate && cd $PROJECT_DIR && python -c \"
from src.training.datamodules import load_test_scenarios
from pathlib import Path
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(name)s %(levelname)s %(message)s')
ds = '${ds}'
print(f'=== Building test cache for {ds} ===', flush=True)
scenarios = load_test_scenarios(ds, Path(f'data/automotive/{ds}'), Path(f'data/cache/{ds}'))
for name, graphs in scenarios.items():
    print(f'  {name}: {len(graphs)} graphs', flush=True)
print(f'=== Done: {ds} ({len(scenarios)} scenarios) ===', flush=True)
\""
    echo "Submitted test-cache job for ${ds}"
done
