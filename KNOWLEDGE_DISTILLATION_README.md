# Knowledge Distillation for CAN-Graph\n\n## Overview\n\nThis implementation provides a complete knowledge distillation system for CAN intrusion detection models. Knowledge distillation allows you to train a smaller, more efficient \"student\" model that learns from a larger, more accurate \"teacher\" model.\n\n## Key Features\n\n✅ **Complete Lightning Integration** - Fully compatible with PyTorch Lightning\n✅ **Memory Optimized** - Teacher output caching and smart memory management\n✅ **Flexible Student Scaling** - Train students at any size (0.5x, 0.75x, 1x, etc.)\n✅ **Comprehensive Monitoring** - Track teacher-student performance gaps\n✅ **Easy Setup** - Interactive setup script and command-line tools\n✅ **Robust Teacher Loading** - Handles both Lightning checkpoints and direct state dicts\n\n## Quick Start\n\n### 1. Prerequisites\n\nMake sure you have a trained teacher model:\n```bash\n# Train a teacher model first (if you don't have one)\npython train_models.py model=gat dataset=hcrl_sa training=normal\n```\n\n### 2. Interactive Setup (Recommended)\n\n```bash\npython setup_distillation.py --interactive\n```\n\nThis will:\n- Show available teacher models\n- Let you select dataset\n- Configure distillation parameters\n- Generate and run the training command\n\n### 3. Quick Setup\n\n```bash\npython setup_distillation.py --quick-setup \\\n    --teacher saved_models/best_teacher_model_hcrl_sa.pth \\\n    --dataset hcrl_sa \\\n    --student-scale 0.5\n```\n\n### 4. Manual Configuration\n\n```bash\n# Using the specialized distillation script\npython train_knowledge_distillation.py \\\n    --teacher_path saved_models/best_teacher_model_hcrl_sa.pth \\\n    --dataset hcrl_sa \\\n    --student_scale 0.5 \\\n    --distillation_alpha 0.7 \\\n    --temperature 4.0 \\\n    --tensorboard\n\n# Or using Hydra configuration\npython train_knowledge_distillation.py \\\n    model=gat dataset=hcrl_sa training=knowledge_distillation \\\n    training.teacher_model_path=saved_models/best_teacher_model_hcrl_sa.pth \\\n    training.student_model_scale=0.5\n```\n\n## File Structure\n\n```\nCAN-Graph/\n├── train_models.py                    # Main training script with KD support\n├── train_knowledge_distillation.py    # Specialized distillation script\n├── setup_distillation.py             # Setup and helper tools\n├── conf/\n│   └── training/\n│       └── knowledge_distillation.yaml # KD configuration\n└── saved_models/\n    ├── *teacher*.pth                  # Teacher models\n    ├── distillation_checkpoints/      # Student training checkpoints\n    └── final_student_model_*.pth      # Final student models\n```\n\n## Configuration Options\n\n### Core Distillation Parameters\n\n- **teacher_model_path**: Path to trained teacher model (required)\n- **student_model_scale**: Scale factor for student size (1.0 = same size, 0.5 = half)\n- **distillation_alpha**: Weight for soft vs hard targets (0.7 = 70% soft, 30% hard)\n- **distillation_temperature**: Softmax temperature for knowledge transfer (4.0 recommended)\n\n### Memory Optimization\n\n```yaml\nmemory_optimization:\n  use_teacher_cache: true              # Cache teacher outputs\n  clear_cache_every_n_steps: 100       # Clear cache periodically\n  offload_teacher_to_cpu: false        # Keep teacher on GPU for speed\n```\n\n### Training Settings\n\n```yaml\nmax_epochs: 80                         # Usually needs fewer epochs\nbatch_size: 32                         # Conservative for memory\nprecision: '16-mixed'                  # Mixed precision for efficiency\naccumulate_grad_batches: 2             # Handle memory pressure\noptimize_batch_size: true              # Auto-optimize batch size\n```\n\n## Understanding the Results\n\n### Training Metrics\n\n- **train_distillation_loss**: Combined loss (soft + hard targets)\n- **hard_loss**: Standard task loss (student vs ground truth)\n- **soft_loss**: Knowledge transfer loss (student vs teacher)\n- **teacher_accuracy**: Teacher model performance\n- **student_accuracy**: Student model performance\n- **accuracy_gap**: Performance difference\n- **teacher_student_similarity**: Output similarity (cosine)\n\n### Model Compression\n\nThe distillation summary shows:\n- Parameter reduction (e.g., \"2.5x smaller\")\n- Memory savings\n- Performance retention\n\n### Example Output\n```\n==============================================================\nKNOWLEDGE DISTILLATION SUMMARY\n==============================================================\nTeacher Model:\n  - Path: saved_models/best_teacher_model_hcrl_sa.pth\n  - Parameters: 2,156,802\n  - Size: 8.23 MB\n\nStudent Model:\n  - Parameters: 862,722\n  - Size: 3.29 MB\n  - Compression: 2.50x smaller\n\nTraining Configuration:\n  - Temperature: 4.0\n  - Alpha: 0.7\n  - Student scale: 0.5\n  - Max epochs: 80\n==============================================================\n```\n\n## Advanced Usage\n\n### Custom Student Architecture\n\nModify the `_scale_student_model()` method in [train_models.py](train_models.py) to implement custom student scaling:\n\n```python\ndef _scale_student_model(self, scale: float):\n    if self.model_type == \"gat\":\n        # Custom scaling logic\n        original_hidden = self.model_config.gat.hidden_channels\n        original_heads = self.model_config.gat.heads\n        \n        # Scale both hidden dimensions and attention heads\n        scaled_hidden = max(8, int(original_hidden * scale))\n        scaled_heads = max(1, int(original_heads * scale))\n        \n        self.model_config.gat.hidden_channels = scaled_hidden\n        self.model_config.gat.heads = scaled_heads\n        \n        self.model = self._create_model()\n```\n\n### Multiple Teacher Ensemble\n\nFor advanced use cases, you can modify the teacher loading to use multiple teachers:\n\n```python\n# In setup_knowledge_distillation()\nteacher_paths = [\n    \"saved_models/teacher1.pth\",\n    \"saved_models/teacher2.pth\"\n]\nself.teacher_models = []\nfor path in teacher_paths:\n    teacher = self._create_model()\n    teacher.load_state_dict(torch.load(path))\n    self.teacher_models.append(teacher)\n```\n\n### Custom Distillation Loss\n\nImplement custom loss functions in `_compute_distillation_loss()`:\n\n```python\ndef _compute_distillation_loss(self, student_output, teacher_output, batch):\n    # Feature matching loss\n    feature_loss = F.mse_loss(student_features, teacher_features)\n    \n    # Attention transfer loss\n    attention_loss = self._attention_transfer_loss(\n        student_attention, teacher_attention\n    )\n    \n    # Combined with standard distillation\n    standard_loss = super()._compute_distillation_loss(\n        student_output, teacher_output, batch\n    )\n    \n    return standard_loss + 0.1 * feature_loss + 0.1 * attention_loss\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Out of Memory**\n   - Reduce `batch_size` in config\n   - Enable `use_teacher_cache: false`\n   - Use `precision: '16-mixed'`\n   - Increase `accumulate_grad_batches`\n\n2. **Teacher Model Loading Failed**\n   - Check teacher model path exists\n   - Use `setup_distillation.py --validate-teacher <path>`\n   - Ensure model architecture matches\n\n3. **Poor Student Performance**\n   - Increase `distillation_temperature` (try 6.0-8.0)\n   - Adjust `distillation_alpha` (try 0.8-0.9)\n   - Reduce `student_model_scale` less aggressively\n   - Train for more epochs\n\n4. **Slow Training**\n   - Enable `use_teacher_cache: true`\n   - Use `precision: '16-mixed'`\n   - Increase batch size if memory allows\n   - Reduce `clear_cache_every_n_steps`\n\n### Performance Tips\n\n- Start with `student_scale: 0.75` before trying smaller models\n- Use `temperature: 4.0` and `alpha: 0.7` as starting points\n- Enable TensorBoard to monitor training: `--tensorboard`\n- Use mixed precision for 2x memory savings\n- Cache teacher outputs for repeated use\n\n## Utilities\n\n### List Available Models\n```bash\npython setup_distillation.py --list-teachers\npython setup_distillation.py --list-datasets\n```\n\n### Validate Teacher\n```bash\npython setup_distillation.py --validate-teacher saved_models/model.pth\n```\n\n### Monitor Training\n```bash\n# TensorBoard (if enabled)\ntensorboard --logdir outputs/distillation_logs\n\n# CSV logs\ncat outputs/distillation_logs/*/metrics.csv\n```\n\n## Integration with Existing Code\n\nThe knowledge distillation system is fully integrated with your existing training pipeline:\n\n- Uses the same data loading and preprocessing\n- Compatible with all model types (GAT, VGAE)\n- Works with your Hydra configuration system\n- Supports the same evaluation and testing workflows\n- Maintains Lightning best practices\n\nYou can switch between normal training and distillation by just changing the training mode:\n\n```bash\n# Normal training\npython train_models.py training=normal\n\n# Knowledge distillation\npython train_models.py training=knowledge_distillation training.teacher_model_path=path/to/teacher.pth\n```\n\n## Next Steps\n\n1. **Train Teacher Models**: Ensure you have well-trained teacher models for each dataset\n2. **Experiment with Scaling**: Try different `student_model_scale` values (0.25, 0.5, 0.75)\n3. **Tune Hyperparameters**: Adjust temperature and alpha based on your specific use case\n4. **Monitor Performance**: Use comprehensive logging to understand teacher-student gaps\n5. **Production Deployment**: Deploy compressed student models for inference\n\n## Examples\n\nSee the `examples/` directory for complete distillation experiment scripts and results analysis.\n