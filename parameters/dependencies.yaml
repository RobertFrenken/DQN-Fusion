# ============================================================================
# CAN-Graph Dependency Schema
#
# Defines prerequisites for each training configuration.
# Used for fail-early validation before job submission.
# ============================================================================

# Level 0: Job Type
job_type:
  single:
    description: "Standalone job - validates prerequisites before submission"
    validation:
      - check_prerequisites_exist
      - fail_early_if_missing

  pipeline:
    description: "Multi-job sequence - manages dependencies automatically"
    validation:
      - build_dependency_graph
      - validate_all_job_paths
      - create_slurm_dependencies

# ============================================================================
# Training Configuration Dependencies
# ============================================================================

prerequisites:
  # --------------------------------------------------------------------------
  # Supervised Learning
  # --------------------------------------------------------------------------
  supervised:
    normal:
      description: "Standard supervised training"
      sample_strategy: "all_samples"  # Normal + attacks
      requires: []  # No prerequisites - baseline

    curriculum:
      description: "Supervised with class imbalance handling"
      sample_strategy: "all_samples"  # Normal + attacks, progressive difficulty
      requires:
        - type: model_checkpoint
          path_pattern: "{modality}/{dataset}/{model_size}/unsupervised/vgae/no-kd/autoencoder"
          reason: "VGAE required for curriculum hard sample mining"
          cli_flag: "--autoencoder-path"
          optional: false

    distillation:
      description: "Knowledge distillation from teacher"
      sample_strategy: "all_samples"
      requires:
        - type: model_checkpoint
          path_pattern: "{modality}/{dataset}/teacher/{learning_type}/{model}/no-kd/{mode}"
          reason: "Teacher model required for knowledge distillation"
          cli_flag: "--teacher-path"
          optional: false
          constraints:
            - model_size: "student"  # Only student can use distillation

  # --------------------------------------------------------------------------
  # Unsupervised Learning
  # --------------------------------------------------------------------------
  unsupervised:
    autoencoder:
      description: "Unsupervised reconstruction training"
      sample_strategy: "normal_only"  # Only normal samples, no attacks
      requires: []  # No prerequisites - baseline

  # --------------------------------------------------------------------------
  # Reinforcement Learning (Fusion)
  # --------------------------------------------------------------------------
  rl_fusion:
    fusion:
      description: "RL-based fusion of pretrained models"
      sample_strategy: "all_samples"
      requires:
        - type: model_checkpoint
          path_pattern: "{modality}/{dataset}/{model_size}/unsupervised/vgae/no-kd/autoencoder"
          reason: "Fusion requires pretrained autoencoder (feature extraction)"
          cli_flag: "--autoencoder-path"
          optional: false

        - type: model_checkpoint
          path_pattern: "{modality}/{dataset}/{model_size}/supervised/{model}/no-kd/{mode}"
          path_glob: "{modality}/{dataset}/{model_size}/supervised/*/no-kd/*"  # Any supervised model
          reason: "Fusion requires pretrained classifier (decision making)"
          cli_flag: "--classifier-path"
          optional: false
          flexible: true  # Can use any supervised model (GAT, GNN, etc.)

# ============================================================================
# Validation Rules (Pâ†’Q Logic)
# ============================================================================

validation_rules:
  # Rule 1: Sample strategy enforcement
  - name: "Sample strategy validation"
    condition: "mode == 'autoencoder'"
    assertion: "use_normal_samples_only == True"
    error_message: "Autoencoder MUST use only normal samples"

  - name: "Sample strategy validation"
    condition: "mode in ['normal', 'curriculum', 'fusion']"
    assertion: "use_all_samples == True"
    error_message: "Supervised/Fusion modes MUST use all samples (normal + attacks)"

  # Rule 2: Distillation constraints
  - name: "Distillation model size constraint"
    condition: "distillation == 'with-kd'"
    assertion: "model_size == 'student'"
    error_message: "Knowledge distillation requires model_size=student"

  - name: "Distillation teacher requirement"
    condition: "distillation == 'with-kd'"
    assertion: "teacher_checkpoint_exists()"
    error_message: "Knowledge distillation requires trained teacher model"

  # Rule 3: Fusion prerequisites
  - name: "Fusion autoencoder requirement"
    condition: "learning_type == 'rl_fusion'"
    assertion: "autoencoder_checkpoint_exists()"
    error_message: "Fusion requires pretrained autoencoder"

  - name: "Fusion classifier requirement"
    condition: "learning_type == 'rl_fusion'"
    assertion: "classifier_checkpoint_exists()"
    error_message: "Fusion requires pretrained classifier"

  # Rule 4: Curriculum VGAE requirement
  - name: "Curriculum VGAE requirement"
    condition: "mode == 'curriculum'"
    assertion: "vgae_checkpoint_exists()"
    error_message: "Curriculum learning requires pretrained VGAE for hard mining"

# ============================================================================
# Path Resolution
# ============================================================================

path_resolution:
  checkpoint_filename: "best.ckpt"
  checkpoint_subdirs: ["checkpoints", "weights", "models"]

  # Flexible matching: If exact path doesn't exist, try alternatives
  fallback_strategy: "glob_match"

  # Example: For fusion classifier prerequisite
  # Exact: automotive/hcrl_ch/teacher/supervised/gat/no-kd/curriculum
  # Glob:  automotive/hcrl_ch/teacher/supervised/*/no-kd/*
  # Matches: curriculum, normal, or any other trained supervised model
