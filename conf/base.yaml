---
epochs: 5
lr: 0.0005
train_ratio: 0.8
root_folder: hcrl_sa
batch_size: 1024
momentum: 0.9
datasize: 1.0
optimizer: torch.optim.SGD
_target_: trainers.DistillationTrainer
device: cuda
use_focal_loss: true
profile: false

# Enhanced fusion parameters for better learning
fusion_episodes: 250            # Slightly reduced with more efficient training
max_train_samples: 150000      # 4x increase (adjust per dataset)
max_val_samples: 30000         # 4x increase  
alpha_steps: 21                # More granular actions: 0.0, 0.033, 0.067, ..., 1.0
fusion_lr: 0.001               # Higher LR for faster learning
fusion_epsilon: 0.9            # Higher exploration initially
fusion_epsilon_decay: 0.995    # Slower decay
fusion_min_epsilon: 0.2       # Higher minimum exploration

# Pipeline parallelism for continuous GPU utilization
fusion_buffer_size: 100000     # A100-optimized buffer (auto-adjusts: 750K/1M based on memory)
fusion_batch_size: 32768        # A100-optimized batches (auto-adjusts: 6K-8K based on memory)  
fusion_target_update: 15       # More frequent updates for continuous training
episode_sample_size: 20000     # Larger episodes to support pipeline processing
training_step_interval: 32     # Smaller intervals for pipeline efficiency
gpu_training_steps: 16         # Increased training steps for pipeline parallelism