# Advanced Lightning training with profiling and optimization
mode: "advanced_lightning"
description: "High-performance Lightning training with profiling"

# Core training parameters
learning_rate: 0.001
weight_decay: 1e-4
scheduler: 
  type: "cosine"
  T_max: 100                    # Total epochs for cosine annealing
  eta_min: 1e-6                 # Minimum learning rate

# Advanced batch configuration  
batch_size: auto                # Lightning Tuner will find optimal
accumulate_grad_batches: 1
gradient_clip_val: 1.0
gradient_clip_algorithm: "norm"

# Performance optimization
compile_model: false            # PyTorch 2.0 compile (experimental)
mixed_precision: "16-mixed"     # Use 16-bit mixed precision for speed
find_unused_parameters: false   # For DDP optimization

# Data loading optimization
dataloader:
  num_workers: auto             # Auto-detect optimal workers
  persistent_workers: true      # Keep workers alive
  pin_memory: true             # Faster GPU transfer
  prefetch_factor: 2           # Prefetch batches
  multiprocessing_context: "spawn"

# Profiling (set to true for performance analysis)
profiler:
  enabled: false
  type: "pytorch"              # pytorch, advanced, simple
  output_dir: "outputs/profiler"
  row_limit: 20

# Memory optimization
detect_anomaly: false          # Enable for debugging
track_grad_norm: true          # Monitor gradient norms
enable_optimization: true

# Callbacks configuration
callbacks:
  model_checkpoint:
    save_top_k: 3
    monitor: "val_loss"
    mode: "min"
    save_last: true
  early_stopping:
    monitor: "val_loss"
    patience: 15
    min_delta: 0.001
    mode: "min"
  lr_monitor:
    logging_interval: "epoch"