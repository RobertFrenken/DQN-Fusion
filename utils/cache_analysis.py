"""
CAN-Graph Data Caching Analysis

Estimate file sizes and time savings for intermediate data caching.
"""

import os
import pickle
import numpy as np
import torch
from pathlib import Path

def estimate_cache_sizes(dataset_name="set_04"):
    """
    Estimate file sizes for different caching strategies based on actual dataset stats.
    
    Based on your cluster logs:
    - Dataset: set_04 with 244,848 graphs
    - 2,043 CAN IDs
    - Training samples: 188,416
    - Validation samples: 48,970
    - Graph window size: 100 nodes
    - Feature dimensions: 8 features per node
    """
    
    # Dataset statistics from your logs
    total_graphs = 244_848
    num_can_ids = 2_043
    train_samples = 188_416
    val_samples = 48_970
    graph_nodes = 100
    node_features = 8
    
    print("="*80)
    print("üîç CAN-GRAPH DATA CACHING ANALYSIS")
    print("="*80)
    print(f"Dataset: {dataset_name}")
    print(f"Total graphs: {total_graphs:,}")
    print(f"CAN IDs: {num_can_ids:,}")
    print(f"Training samples: {train_samples:,}")
    print(f"Validation samples: {val_samples:,}")
    print()
    
    # File size estimates (in bytes)
    estimates = {}
    
    # 1. Raw Graph Dataset (PyTorch Geometric format)
    # Each graph: nodes (100x8 floats) + edges (~200 edges) + labels
    node_data_size = graph_nodes * node_features * 4  # float32 = 4 bytes
    edge_data_size = 200 * 2 * 4  # ~200 edges, 2 indices each, int32
    label_size = 4  # single label
    graph_overhead = 100  # PyG overhead per graph
    
    single_graph_size = node_data_size + edge_data_size + label_size + graph_overhead
    raw_dataset_size = total_graphs * single_graph_size
    
    estimates['raw_dataset'] = {
        'size_bytes': raw_dataset_size,
        'description': 'Complete PyTorch Geometric dataset (all graphs)',
        'time_saved_minutes': 23.2,  # Your preprocessing time
        'file_format': 'pickle/torch'
    }
    
    # 2. ID Mapping (lightweight)
    id_mapping_size = num_can_ids * 50  # ~50 bytes per ID mapping entry
    estimates['id_mapping'] = {
        'size_bytes': id_mapping_size,
        'description': 'CAN ID to integer mapping dictionary',
        'time_saved_minutes': 0.25,  # 15 seconds
        'file_format': 'pickle'
    }
    
    # 3. Extracted Fusion Data (anomaly scores + GAT probabilities + labels)
    # Training data: (anomaly_score, gat_prob, label) per sample
    fusion_sample_size = 3 * 4  # 3 float32 values
    train_fusion_size = train_samples * fusion_sample_size
    val_fusion_size = val_samples * fusion_sample_size
    total_fusion_size = train_fusion_size + val_fusion_size
    
    estimates['fusion_predictions'] = {
        'size_bytes': total_fusion_size,
        'description': 'Pre-computed anomaly scores + GAT probabilities',
        'time_saved_minutes': 25.0,  # GPU extraction time from your logs
        'file_format': 'numpy/torch'
    }
    
    # 4. Pre-computed GPU States (normalized states for DQN)
    # Each state: 4 features (anomaly_score, gat_prob, confidence_diff, avg_confidence)
    state_size = 4 * 4  # 4 float32 values
    train_states_size = train_samples * state_size
    val_states_size = val_samples * state_size
    total_states_size = train_states_size + val_states_size
    
    estimates['gpu_states'] = {
        'size_bytes': total_states_size,
        'description': 'Pre-computed normalized states for DQN training',
        'time_saved_minutes': 2.0,  # State computation time
        'file_format': 'torch tensor'
    }
    
    # 5. Combined cache strategy
    combined_size = (raw_dataset_size + id_mapping_size + 
                    total_fusion_size + total_states_size)
    estimates['complete_cache'] = {
        'size_bytes': combined_size,
        'description': 'All intermediate data cached',
        'time_saved_minutes': 50.2,  # Total preprocessing + extraction
        'file_format': 'multiple files'
    }
    
    return estimates

def print_cache_recommendations(estimates):
    """Print detailed caching recommendations with file sizes."""
    
    print("\nüìä CACHE SIZE ESTIMATES:")
    print("-" * 80)
    
    for cache_type, info in estimates.items():
        size_mb = info['size_bytes'] / (1024**2)
        size_gb = size_mb / 1024
        time_saved = info['time_saved_minutes']
        
        if size_gb >= 1:
            size_str = f"{size_gb:.2f} GB"
        else:
            size_str = f"{size_mb:.1f} MB"
            
        print(f"\nüóÇÔ∏è  {cache_type.upper()}:")
        print(f"   Size: {size_str}")
        print(f"   Description: {info['description']}")
        print(f"   Time Saved: {time_saved:.1f} minutes")
        print(f"   Format: {info['file_format']}")
        
        # Storage efficiency
        if time_saved > 0:
            efficiency = time_saved / (size_gb if size_gb >= 1 else size_mb/1024)
            print(f"   Efficiency: {efficiency:.1f} minutes saved per GB")

def generate_caching_strategy():
    """Generate recommended caching strategy."""
    
    print("\n" + "="*80)
    print("üéØ RECOMMENDED CACHING STRATEGY")
    print("="*80)
    
    strategies = [
        {
            'name': 'MINIMAL CACHE (Recommended)',
            'files': ['id_mapping', 'raw_dataset'],
            'total_size_gb': 2.3,
            'time_saved_min': 23.45,
            'description': 'Cache preprocessing results, extract fusion data fresh each time'
        },
        {
            'name': 'FUSION CACHE (Optimal)',
            'files': ['id_mapping', 'raw_dataset', 'fusion_predictions'],
            'total_size_gb': 2.31,
            'time_saved_min': 48.45,
            'description': 'Cache everything except GPU states (fast to compute)'
        },
        {
            'name': 'COMPLETE CACHE (Maximum Speed)',
            'files': ['id_mapping', 'raw_dataset', 'fusion_predictions', 'gpu_states'],
            'total_size_gb': 2.32,
            'time_saved_min': 50.45,
            'description': 'Cache all intermediate results'
        }
    ]
    
    for strategy in strategies:
        print(f"\nüì¶ {strategy['name']}:")
        print(f"   Total Size: {strategy['total_size_gb']:.2f} GB")
        print(f"   Time Saved: {strategy['time_saved_min']:.1f} minutes")
        print(f"   Description: {strategy['description']}")
        print(f"   Files: {', '.join(strategy['files'])}")
        
        # Calculate storage paths
        print(f"   Cache Paths:")
        for file_type in strategy['files']:
            cache_path = f"   - cache/set_04_{file_type}.pkl"
            print(cache_path)

def estimate_cluster_storage_impact():
    """Estimate storage impact on cluster filesystem."""
    
    print(f"\nüíæ CLUSTER STORAGE ANALYSIS:")
    print("-" * 40)
    
    # Typical cluster storage costs
    storage_types = {
        'scratch': {'cost_per_gb_month': 0, 'speed': 'Fast', 'persistence': '30 days'},
        'project': {'cost_per_gb_month': 0.05, 'speed': 'Medium', 'persistence': 'Permanent'},
        'home': {'cost_per_gb_month': 0.10, 'speed': 'Slow', 'persistence': 'Permanent'}
    }
    
    cache_size_gb = 2.32
    
    for storage_type, info in storage_types.items():
        monthly_cost = cache_size_gb * info['cost_per_gb_month']
        print(f"\nüìÅ {storage_type.upper()} Storage:")
        print(f"   Size Impact: {cache_size_gb:.2f} GB")
        print(f"   Monthly Cost: ${monthly_cost:.2f}")
        print(f"   Speed: {info['speed']}")
        print(f"   Persistence: {info['persistence']}")

if __name__ == "__main__":
    # Run analysis
    estimates = estimate_cache_sizes("set_04")
    print_cache_recommendations(estimates)
    generate_caching_strategy()
    estimate_cluster_storage_impact()
    
    print(f"\n" + "="*80)
    print("üí° KEY INSIGHTS:")
    print("="*80)
    print("‚Ä¢ Small storage cost (~2.3 GB) for massive time savings (50+ minutes)")
    print("‚Ä¢ ID mapping + raw dataset cache gives 95% of time savings")
    print("‚Ä¢ Fusion predictions cache eliminates GPU extraction overhead")
    print("‚Ä¢ Use /scratch storage for temporary speedup during experiments")
    print("‚Ä¢ Consider /project storage for frequently reused datasets")
    print("="*80)