---
title: "Appendix"
---

## Experimental Setup

#### Implementation Details

80% of the dataset was utilized for training, 20% for validation, and a distinct test set was compiled by the dataset providers. All experiments were conducted using PyTorch and PyTorch Geometric. Model training and evaluation were performed on GPU clusters provided by the Ohio Supercomputer Center (OSC) [@OhioSupercomputerCenter1987]. Each CAN message is represented with 11 features (CAN ID, 8 data bytes, message count, and bus position).

## Model Sizing for Cascading Knowledge Distillation Ensemble {#sec-model-sizing}

This section provides specific parameter budgets for the three-model student ensemble (GAT classifier, VGAE autoencoder, DQN fusion) derived from the CAN bus latency constraints, with unequal allocation reflecting architectural complexity and inference cost trade-offs.

#### Total Parameter Budget

From the CAN bus latency constraint (7 ms hard limit [@ARM-Cortex-A7-TRM]), the total onboard parameter budget is:

$$
N_{\text{onboard, total}} = 173\text{ K parameters (FP32)}
$$

Using the empirical distillation scaling law with target compression ratio $\kappa \approx 20$:

$$
N_{t,\text{model}} \approx 20 \times N_{s,\text{model}} \quad \text{for each model}
$$

#### Heterogeneous Model Allocation

Student ensemble members are not equally sized. The GAT classifier and VGAE autoencoder perform primary detection tasks and receive larger parameter budgets, while the DQN fusion model aggregates their outputs and receives reduced allocation:

| Model | Student | Teacher | Compression |
|-------|---------|---------|-------------|
| GAT Classifier | 55 K | 1.100 M | 20$\times$ |
| VGAE Autoencoder | 86 K | 1.710 M | $\approx$20$\times$ |
| DQN Fusion | 32 K | 687 K | $\approx$21$\times$ |
| **Total (Onboard)** | **173 K** | --- | --- |
| **Total (Offline)** | --- | **3.497 M** | --- |

: Parameter Budget Allocation Across Student and Teacher Ensembles {#tbl-model-allocation}

### Model Architecture Details

@tbl-teacher-student details the architectural parameters for all teacher and student models across the three ensemble members. Each model follows a teacher-student distillation framework with $\approx 20\times$ compression ratio.

```{python}
#| label: tbl-teacher-student
#| tbl-cap: "Teacher (Tch.) and Student (Std.) Model Parameters for Classifier, Autoencoder, and Fusion"
import pandas as pd
from IPython.display import Markdown

df = pd.read_csv("data/model_params.csv")
Markdown(df.to_markdown(index=False))
```

@fig-model-sizes visualizes the parameter counts across all model variants, illustrating the compression achieved through knowledge distillation.

```{ojs}
//| label: fig-model-sizes
//| fig-cap: "Parameter counts by model type and scale. Large models serve as teachers; small models are the distilled students deployed on edge hardware."

model_sizes_json = FileAttachment("../data/model_sizes.json").json()

{
  const data = model_sizes_json.data;
  return Plot.plot({
    width: 500,
    height: 280,
    marginLeft: 100,
    x: { label: "Parameters (millions)" },
    y: { label: null },
    color: { legend: true, domain: ["large", "small"], range: ["#58a6ff", "#79c0ff"] },
    marks: [
      Plot.barX(data, {
        y: d => `${d.model_type} (${d.scale})`,
        x: "param_count_M",
        fill: "scale",
        sort: { y: "-x" }
      }),
      Plot.text(data, {
        y: d => `${d.model_type} (${d.scale})`,
        x: "param_count_M",
        text: d => `${d.param_count_M}M`,
        dx: 5,
        textAnchor: "start",
        fontSize: 11
      }),
      Plot.ruleX([0])
    ]
  });
}
```

#### GAT Classifier (55 K Student, 1.100 M Teacher)

Graph attention network over 11 CAN signal features per node (CAN ID, 8 data bytes, message count, and bus position). The student model uses 2 stacked GAT layers with 4 attention heads and 24 hidden channels, producing 8-dimensional node embeddings that feed into a 2-class classification head. The teacher model expands to 5 GAT layers with 8 attention heads and 64 hidden channels with 32-dimensional embeddings, using jumping knowledge (concatenation mode) and residual connections for higher representational capacity and softer knowledge targets during distillation.

#### VGAE Autoencoder (86 K Student, 1.710 M Teacher)

Variational graph autoencoder for unsupervised anomaly detection. The student model compresses 11-dimensional input features through a 2-layer GATConv encoder (single attention head, progressive schedule $[80 \to 40]$) to a 16-dimensional latent space, with a symmetric decoder for reconstruction. The teacher uses a deeper encoder with 4 attention heads and a progressive hidden schedule $[1024 \to 512]$ to a 96-dimensional latent space, with 64-dimensional CAN ID embeddings and a 256-unit MLP neighborhood decoder for richer representation learning. Both models employ the variational reparameterization trick.

#### DQN Fusion (32 K Student, 687 K Teacher)

Deep Q-Network for multi-model state fusion, aggregating features from the VGAE autoencoder (8D: 3 reconstruction error components, 4 latent statistics, 1 confidence score) and GAT classifier (7D: 2 logits, 4 embedding statistics, 1 confidence score) into a combined 15D input state vector. The student model uses 2 hidden layers (160 units each), while the teacher expands to 3 hidden layers (576 units each). Both use Double DQN with LayerNorm and dropout regularization. The action space $|A| = 21$ corresponds to uniformly spaced fusion weights $\alpha \in [0, 1]$.

### GAT Architecture

The teacher GAT uses 5 GATConv layers with 8 attention heads, 64 hidden channels, and 32-dimensional node embeddings. Jumping knowledge (concatenation mode) and residual connections aggregate multi-scale features into a single-layer FC classification head (2 outputs: normal/attack). Dropout is set to 0.2.

The student GAT uses 2 GATConv layers with 4 attention heads, 24 hidden channels, and 8-dimensional embeddings. It omits jumping knowledge and residual connections for deployment simplicity, using a 2-layer FC head instead. Dropout is reduced to 0.1.

### VGAE Architecture

Our VGAE encoder progressively compresses the input through multiple GATConv layers.

**Teacher:** Starting with 11-dimensional CAN features and 64-dimensional CAN ID embeddings, we apply:

- Layer 1: GATConv($11 \to 1024$, heads=4) with multi-head attention
- Layer 2: GATConv($1024 \to 512$, heads=4) with multi-head refinement
- Bottleneck: Linear projection to latent distribution ($\mu$, $\sigma$ for 96-d $\mathbf{z}$)

The decoder mirrors this structure, reconstructing the 11 continuous features from the sampled latent code. The CAN ID is separately classified from the latent representation via a 256-unit MLP decoder.

**Student:** Starting with 11-dimensional CAN features and 4-dimensional CAN ID embeddings:

- Layer 1: GATConv($11 \to 80$, heads=1) with single-head attention
- Layer 2: GATConv($80 \to 40$, heads=1) with single-head refinement
- Bottleneck: Linear projection to latent distribution ($\mu$, $\sigma$ for 16-d $\mathbf{z}$)

The student decoder mirrors the encoder ($[40, 80]$) and uses a compact 16-unit MLP neighborhood decoder.

### DQN Architecture

**Teacher:** The teacher DQN takes a 15-dimensional state vector (8 VGAE features + 7 GAT features) as input. The state is composed of:

- VGAE features (8D): 3 reconstruction error components (node, neighbor, CAN ID), 4 latent statistics (mean, std, max, min of $\mathbf{z}$), and 1 confidence score
- GAT features (7D): 2 class logits, 4 embedding statistics (mean, std, max, min), and 1 confidence score

Architecture: 3 hidden layers with 576 units each, LayerNorm, ReLU, and 0.2 dropout. Output: 21 Q-values corresponding to $\alpha \in \{0, 0.05, \ldots, 1.0\}$. Uses Double DQN with target network updates every 100 steps.

**Student:** Same 15D input state. Architecture: 2 hidden layers with 160 units each, LayerNorm, ReLU, and 0.1 dropout. Lower exploration ($\epsilon=0.05$, faster decay) and more frequent target updates (every 50 steps) for stable deployment.

## Distillation Training

The GAT and VGAE students train using knowledge distillation from their respective teachers. The distillation loss is a weighted combination of the task loss and the KD loss:

$$
L_{\text{total}} = \alpha \, L_{\text{KD}} + (1 - \alpha) \, L_{\text{task}}
$$ {#eq-appendix-kd-loss}

where $\alpha = 0.7$ and $L_{\text{KD}}$ employs temperature-scaled softmax with temperature $T = 4.0$:

$$
L_{\text{KD}} = T^{2} \cdot \text{KL}\!\left( \frac{\log \hat{y}_{s}}{T} \;\middle\|\; \frac{\hat{y}_{t}}{T} \right)
$$

For the VGAE, distillation combines latent-space alignment and reconstruction matching:

$$
L_{\text{KD}}^{\text{VGAE}} = 0.5 \, L_{\text{latent}} + 0.5 \, L_{\text{recon}}
$$

where $L_{\text{latent}}$ is the MSE between student and teacher latent representations (with a learned projection when dimensions differ) and $L_{\text{recon}}$ is the MSE between continuous output reconstructions.

## Inference Cost

Combined student ensemble inference (all three models):

$$
\text{FLOPs}_{\text{inference}} = (55\text{ K} + 86\text{ K} + 32\text{ K}) \times 2 = 346\text{ K FLOPs}
$$

$$
\text{Latency}_{\text{inference}} = \frac{346 \text{ K FLOPs}}{50 \text{ MFLOP/s}} \times 0.7 \text{ (system overhead)} = 4.8\text{ ms}
$$

This provides $\approx 2.2$ ms safety margin within the 7 ms CAN message cycle (100 Hz), accounting for context switches, cache misses, and interrupt handling.

## Reproducibility

This work used FP32 for training stability, model interpretability, and benchmark reproducibility. INT8 quantization on student models (providing $\approx 2.1\times$ speedup on ARM Cortex-A7) would enable approximately $2.5\times$ parameter expansion while maintaining the same 4.8 ms inference latency. Current work focuses on FP32.
