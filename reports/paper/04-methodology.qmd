---
title: "Methodology"
---

## Methodology Overview

The proposed framework employs a three-stage pipeline for robust intrusion detection in Controller Area Network (CAN) bus systems. Stage 1 uses a Variational Graph Autoencoder (VGAE) to identify hard examples; Stage 2 trains a Graph Attention Network (GAT) with curriculum learning on filtered samples; Stage 3 leverages a Deep Q-Network (DQN) to learn dynamic fusion weights combining VGAE and GAT predictions. The workflow supports both training (sequential stages) and inference (parallel GAT/VGAE outputs fused by DQN).

### Graph Construction

CAN messages are broadcast by Electronic Control Units (ECUs); CAN IDs identify message types and are not unique per packet---multiple ECUs can transmit the same ID, and any ECU can receive all messages. This broadcast model underpins the graph representation, capturing sequential dependencies within the CAN stream.

::: {#alg-graph-construction}
#### Algorithm 1: Graph Construction from CAN Stream {.unnumbered}

**Require:** CAN stream $M = \{m_t = (\text{ID}_t, \text{payload}_t)\}$, window size $W$
**Ensure:** Graphs $\mathcal{G} = \{G_t = (V_t, E_t, X_t, y_t)\}$

1. **for** $t = W$ **to** $|M|$ **do**
    a. $W_t \leftarrow M[t-W+1 : t]$ $\triangleright$ extract window
    b. $\text{source} \leftarrow W_t[:, -3]$; $\text{target} \leftarrow W_t[:, -2]$ $\triangleright$ CAN IDs
    c. $\text{edges} \leftarrow \text{stack}(\text{source}, \text{target})$
    d. $(\text{unique\_edges}, \text{counts}) \leftarrow \text{unique}(\text{edges})$ $\triangleright$ transitions
    e. $V_t \leftarrow \text{unique}(\text{source} \cup \text{target})$ $\triangleright$ unique nodes
    f. $\text{node\_map} \leftarrow \{v \mapsto \text{idx} \mid v \in V_t\}$ $\triangleright$ node indexing
    g. $E_t \leftarrow [(\text{node\_map}[\text{src}], \text{node\_map}[\text{tgt}])$ for $(\text{src}, \text{tgt}) \in \text{unique\_edges}]$
    h. Compute node features: $X_t \in \mathbb{R}^{|V_t| \times 11}$
    i. Compute edge features: $F_t \in \mathbb{R}^{|E_t| \times 11}$
    j. $y_t \leftarrow 1$ if any attack ID $\in W_t$ else $0$ $\triangleright$ label
2. **end for**
:::

@fig-can-graph visualizes a sample CAN bus graph constructed using this algorithm, rendered as a force-directed layout where nodes represent unique CAN IDs and edges encode sequential transitions within a traffic window.

```{ojs}
//| label: fig-can-graph
//| fig-cap: "Force-directed CAN bus graph structure. Nodes represent unique CAN IDs within a traffic window; edges encode sequential message transitions. Node size reflects degree centrality."

import { renderForceGraph } from "../_ojs/force-graph.js"

graph_samples_json = FileAttachment("../data/graph_samples.json").json()

{
  const d3 = await import("https://cdn.jsdelivr.net/npm/d3@7/+esm");
  const container = document.createElement("div");
  container.style.position = "relative";
  container.style.width = "100%";

  const style = document.createElement("style");
  style.textContent = `
    .tooltip {
      position: absolute;
      pointer-events: none;
      background: rgba(13, 17, 23, 0.95);
      color: #e6edf3;
      border: 1px solid #30363d;
      border-radius: 6px;
      padding: 8px 12px;
      font-size: 12px;
      opacity: 0;
      transition: opacity 0.15s;
      z-index: 10;
    }
  `;
  container.appendChild(style);

  const datasets = [...new Set(graph_samples_json.data.map(d => d.dataset))].sort();
  renderForceGraph(d3, container, graph_samples_json.data, {
    dataset: datasets[0],
    colorBy: "can_id",
    width: 700,
    height: 450,
  });

  return container;
}
```

Node features (11 dimensions) include: CAN ID, normalized payload bytes (mean across occurrences), occurrence count, and temporal position (last occurrence normalized to window length). Edge features (11 dimensions) comprise: frequency count, relative frequency, mean/std inter-arrival intervals, regularity score, temporal position metrics (first, last, spread), reverse-edge indicator, degree product, and degree ratio. Window size $W=100$ balances temporal context and computational efficiency. Graphs are directed and weighted by transition counts; self-loops occur for consecutive identical IDs.

### Training Paradigm

#### Stage 1: VGAE Training and Hard Sample Selection

::: {#alg-hard-sample}
#### Algorithm 2: VGAE-Based Hard Sample Selection {.unnumbered}

**Require:** Trained VGAE model on normal graphs
**Ensure:** Hard-selected training dataset for Stage 2

1. Train VGAE on normal graphs until convergence
2. **for each** normal graph $G_i$ **do**
    a. $R_i \leftarrow \|A_i - \hat{A}_i\|_F^2 / |V_i|^2$ $\triangleright$ reconstruction error
3. **end for**
4. Rank by $R_i$ in descending order; select top-$k$ as hard negatives
5. Combine hard normal samples with all attack samples
:::

High reconstruction error indicates ambiguous or boundary-proximate normal samples. Selective undersampling preserves discriminative hard examples while reducing majority class dominance.

#### Stage 2: GAT Training with Curriculum Learning

**Curriculum Learning:** Momentum-based scheduler transitions from balanced to imbalanced sampling:

$$
p_t = 1 - \exp(-t / \tau)
$$ {#eq-curriculum-momentum}

Batch composition blends three sources:

$$
B_t = (1 - p_t) B_{\text{bal}} + p_t B_{\text{nat}} + \alpha_{\text{buf}} B_{\text{hard}}
$$ {#eq-batch-composition}

where $B_{\text{bal}}$ is class-balanced, $B_{\text{nat}}$ reflects natural imbalance, $B_{\text{hard}}$ contains highest-error samples from VGAE buffer ($\alpha_{\text{buf}} = 0.2$), and buffer is refreshed every 100 steps. This prevents premature majority bias while maintaining natural distribution awareness.

**Knowledge Distillation:** Student GAT mimics pre-trained teacher via logit-level distillation:

$$
\mathcal{L}_{\text{KD}} = (1 - \lambda) \mathcal{L}_{\text{CE}} + \lambda T^2 \text{KL}(p_{\text{student}} \| p_{\text{teacher}})
$$ {#eq-kd-loss}

where $T$ is temperature (softening factor) and $\lambda$ is mixing coefficient. No intermediate feature distillation applied.

#### Stage 3: DQN-Based Adaptive Fusion

After Stages 1--2, a Deep Q-Network learns optimal fusion weights combining VGAE and GAT predictions via supervised reinforcement learning. Training uses ground truth labels to compute reward signals.

**State Space:** 15-dimensional feature vector aggregating VGAE and GAT outputs: VGAE reconstruction errors (node, neighbor, CAN ID levels), latent space statistics (mean, std, max, min), VGAE confidence; GAT logits (class 0, class 1), embedding statistics (mean, std, max, min), GAT confidence. All features normalized and clipped to $[0,1]$.

**Action Space:** $N=21$ discrete fusion weights linearly spaced in $[0,1]$. Policy semantics: $\alpha = 0.5$ (equal weighting), $\alpha < 0.5$ (favor VGAE), $\alpha > 0.5$ (favor GAT). Fused anomaly score: $\sigma = (1 - \alpha) \cdot \text{VGAE\_anomaly} + \alpha \cdot \text{GAT\_prob}$; final prediction $\hat{y} = \mathbb{1}[\sigma > 0.5]$.

**Reward Function:** Directly tied to classification accuracy using ground truth labels (supervised RL):

$$
R(\hat{y}, y_{\text{true}}, s, \alpha) = \begin{cases}
    +3.0 + r_{\text{agree}} + r_{\text{conf}} & \text{if } \hat{y} = y_{\text{true}} \\
    -3.0 + r_{\text{disagree}} + r_{\text{overconf}} & \text{if } \hat{y} \neq y_{\text{true}}
\end{cases}
$$ {#eq-reward}

where $r_{\text{agree}}$ measures alignment between VGAE and GAT (model agreement bonus), $r_{\text{conf}}$ rewards high confidence on correct predictions, $r_{\text{disagree}}$ penalizes misalignment on errors, $r_{\text{overconf}}$ penalizes overconfidence on incorrect predictions, and implicit balance bonus discourages extreme $\alpha$ values.

### Inference Pipeline

At inference, VGAE and GAT execute in parallel to minimize latency. Their outputs are concatenated into the 15D state vector and passed to the DQN for dynamic weight determination and final prediction. Parallelization of VGAE and GAT ensures both models evaluate concurrently, while the DQN adds minimal overhead (single forward pass through a small fully-connected network). This design enables real-time deployment in resource-constrained CAN bus environments with sub-millisecond inference latency.
