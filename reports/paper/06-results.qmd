---
title: "Results and Discussion"
---

{{< include _setup.qmd >}}

## Results and Discussion

### Test Set Performance

@tbl-results-car and @tbl-results-synth summarize the test set performance across six datasets. We compare against four GNN-based baselines: KD-GAT [@frenken2025kdgat], A&D GAT [@AD], G-IDCS [@Park], and GUARD-CAN [@guardcan2025]. KD-GAT serves as the primary baseline since it is the only method evaluated on the comprehensive can-train-and-test dataset [@Lampe2024cantrainandtest].

Our approach demonstrates consistent improvements across all datasets, with particularly significant gains on highly imbalanced datasets. Compared to KD-GAT, we achieve an average improvement of 2.09% in accuracy and 16.22% in F1-score. The most substantial improvements occur on challenging datasets S02 and S04, where F1-scores improve by 55.25% and 30.64% respectively, indicating superior handling of severe class imbalance.

| Method | CarH Acc | CarH F1 | CarS Acc | CarS F1 |
|--------|----------|---------|----------|---------|
| A&D | 99.95 | 99.94 | -- | -- |
| G-IDCS | 97.25 | 93.36 | -- | -- |
| GUARD-CAN | 97.02 | 97.27 | -- | -- |
| KD-GAT | 99.97 | 99.97 | 99.31 | 99.29 |
| **Ours** | **99.89** | **99.89** | **99.96** | **99.96** |

: Test Set Performance on CarH and CarS {#tbl-results-car}

| Method | S01 Acc | S01 F1 | S02 Acc | S02 F1 | S03 Acc | S03 F1 | S04 Acc | S04 F1 |
|--------|---------|--------|---------|--------|---------|--------|---------|--------|
| KD-GAT | 99.29 | 88.08 | 98.18 | 24.42 | 98.24 | 86.06 | 87.07 | 61.35 |
| **Ours** | **99.38** | **89.86** | **99.61** | **79.67** | **99.29** | **95.10** | **96.47** | **91.99** |

: Test Set Performance on CAN-train-and-test Datasets (S01--S04) {#tbl-results-synth}

### Knowledge Distillation Effectiveness

@fig-kd-transfer visualizes the knowledge transfer between teacher (large) and student (small + KD) models. Points near the diagonal indicate successful knowledge distillation with minimal performance degradation.

```{ojs}
//| label: fig-kd-transfer
//| fig-cap: "Knowledge distillation transfer: teacher (large) vs student (small + KD) performance. Points near the diagonal indicate effective knowledge transfer. Color encodes dataset."

kd_transfer_json = FileAttachment("../data/kd_transfer.json").json()

viewof selectedKdMetric = Inputs.select(
  ["f1", "accuracy", "auc"],
  { label: "Metric", value: "f1" }
)

{
  const data = kd_transfer_json.data.filter(d => d.metric_name === selectedKdMetric);
  const metricLabel = selectedKdMetric.toUpperCase();
  return Plot.plot({
    width: 600,
    height: 360,
    x: { label: `Teacher ${metricLabel} (Large)`, domain: [0.5, 1.01] },
    y: { label: `Student ${metricLabel} (Small + KD)`, domain: [0.5, 1.01] },
    color: { legend: true },
    marks: [
      Plot.line([[0.5, 0.5], [1, 1]], { stroke: "#8b949e", strokeDasharray: "4,4", strokeWidth: 1 }),
      Plot.dot(data, {
        x: "teacher_value",
        y: "student_value",
        fill: "dataset",
        stroke: "dataset",
        r: 7,
        tip: true,
        title: d => `${d.dataset} / ${d.model_type}\nTeacher: ${d.teacher_value.toFixed(4)}\nStudent: ${d.student_value.toFixed(4)}`
      }),
      Plot.text(data, {
        x: "teacher_value",
        y: "student_value",
        text: d => d.model_type.slice(0, 3),
        dy: -12,
        fontSize: 10
      })
    ]
  });
}
```

### Discussion

**Class Imbalance Handling:** Our multi-stage approach demonstrates superior performance on imbalanced datasets compared to single-stage methods. The VGAE component effectively captures structural anomalies even with limited attack samples, while the GAT classifier benefits from the refined feature representations. This combination proves particularly effective on datasets S02 and S04, where traditional methods struggle with extreme class ratios.

**Generalization Capability:** The consistent performance across diverse datasets (CarH, CarS, and can-train-and-test subsets) demonstrates strong generalization. Unlike previous methods that show significant performance degradation on unseen test data, our approach maintains robust detection capabilities across different attack types and network conditions.
