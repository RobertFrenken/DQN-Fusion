---
title: "Explainability"
---

{{< include _setup.qmd >}}

## Explainability

### UMAP Analysis

To understand the representations learned by our model, we perform UMAP-based feature analysis using both raw input statistics and learned graph embeddings. We sample 10% of graphs from the HCRL Car-Hacking dataset.

Raw CAN-graph data projected via UMAP shows loose clustering that indicates limited separability between normal and attack types. In contrast, UMAP projections of graph-level embeddings from the trained GAT classifier's penultimate layer reveal well-separated clusters.

Despite binary supervision (attack vs. normal), the learned embedding space forms well-separated clusters aligned with specific attack types (DoS, Fuzzy, Gear, RPM). This emergent multi-class structure demonstrates that our model captures high-level semantic patterns in CAN traffic and generalizes across attack categories without explicit multi-class labels. The clear cluster separation in embedding space, absent in raw features, validates the GAT's ability to learn discriminative representations from graph-structured temporal data.

```{ojs}
//| label: fig-umap-vgae
//| fig-cap: "UMAP projection of VGAE latent embeddings. Normal samples (green) and attack samples (red) show separation patterns learned by the autoencoder without explicit labels."
{
  await loadTable("embeddings", FileAttachment("../data/embeddings.parquet"));
  const sel = vg.Selection.single();
  return vg.vconcat(
    vg.menu({ from: "embeddings", column: "run_id", label: "Run", as: sel }),
    vg.plot(
      vg.dot(
        vg.from("embeddings", { filterBy: sel }),
        {
          x: "x",
          y: "y",
          fill: vg.sql`CASE WHEN label = 0 THEN 'normal' ELSE 'attack' END`,
          r: 2,
          opacity: 0.5
        }
      ),
      vg.width(600),
      vg.height(350),
      vg.colorLegend(true),
      vg.colorDomain(["normal", "attack"]),
      vg.colorRange(["#3fb950", "#f85149"]),
      vg.xLabel("UMAP 1"),
      vg.xAxis(null),
      vg.yLabel("UMAP 2"),
      vg.yAxis(null)
    )
  );
}
```

```{ojs}
//| label: fig-umap-gat
//| fig-cap: "UMAP projection of GAT penultimate-layer embeddings. The supervised classifier produces well-separated clusters, with emergent multi-class structure despite binary training."
{
  const sel = vg.Selection.single();
  return vg.vconcat(
    vg.menu({ from: "embeddings", column: "run_id", label: "Run", as: sel }),
    vg.plot(
      vg.dot(
        vg.from("embeddings", { filterBy: sel }),
        {
          x: "x",
          y: "y",
          fill: vg.sql`CASE WHEN label = 0 THEN 'normal' ELSE 'attack' END`,
          r: 2,
          opacity: 0.5
        }
      ),
      vg.width(600),
      vg.height(350),
      vg.colorLegend(true),
      vg.colorDomain(["normal", "attack"]),
      vg.colorRange(["#3fb950", "#f85149"]),
      vg.xLabel("UMAP 1"),
      vg.xAxis(null),
      vg.yLabel("UMAP 2"),
      vg.yAxis(null)
    )
  );
}
```

### Composite VGAE Reconstruction Error

To assess the overall reconstruction quality of the VGAE, we combine three types of reconstruction errors: node feature reconstruction error ($E_{\text{node}}$), CAN ID prediction error ($E_{\text{CAN\,ID}}$), and neighborhood reconstruction error ($E_{\text{neighbor}}$). Each error captures a different aspect of the graph structure and message semantics. We compute a single composite score as a weighted sum:

$$
\mathrm{Composite\_Error} = \alpha\, E_{\text{node}} + \beta\, E_{\text{neighbor}} + \gamma\, E_{\text{CAN\,ID}}
$$ {#eq-composite-error}

where $\alpha$, $\beta$, and $\gamma$ are empirically chosen weights that regulate each term's influence. In our experiments, we use $\alpha = 1.0$, $\beta = 20.0$, and $\gamma = 0.3$.

This approach enables the detection of subtle anomalies by jointly evaluating node content, CAN identifier semantics, and local neighborhood structure.

@fig-recon-errors shows the distribution of composite reconstruction errors for normal and attack samples. The separation between distributions validates the VGAE's ability to discriminate between normal traffic patterns and anomalous injections.

```{ojs}
//| label: fig-recon-errors
//| fig-cap: "Distribution of VGAE composite reconstruction errors. Normal samples (green) concentrate at low error values, while attack samples (red) exhibit higher reconstruction errors, enabling threshold-based anomaly detection."

{
  await loadTable("recon_errors", FileAttachment("../data/recon_errors.parquet"));
  const sel = vg.Selection.single();
  return vg.vconcat(
    vg.menu({ from: "recon_errors", column: "run_id", label: "Run", as: sel }),
    vg.plot(
      vg.rectY(
        vg.from("recon_errors", { filterBy: sel }),
        {
          x: vg.bin("error"),
          y: vg.count(),
          fill: vg.sql`CASE WHEN label = 0 THEN 'normal' ELSE 'attack' END`,
          insetLeft: 0.5,
          insetRight: 0.5
        }
      ),
      vg.width(650),
      vg.height(320),
      vg.colorLegend(true),
      vg.colorDomain(["normal", "attack"]),
      vg.colorRange(["#3fb950", "#f85149"]),
      vg.xLabel("Reconstruction Error"),
      vg.yLabel("Count")
    )
  );
}
```

### Attention Weight Analysis

To understand which graph structural features the GAT prioritizes, we analyze the learned attention weights across layers and heads. @fig-attention-heatmap shows the mean attention weight distribution, revealing how different heads specialize in capturing distinct message relationship patterns.

```{ojs}
//| label: fig-attention-heatmap
//| fig-cap: "GAT attention weight heatmap across layers and heads. Color intensity indicates mean attention magnitude. Different heads learn to attend to different structural patterns in the CAN bus graph."

{
  await loadTable("attention_weights", FileAttachment("../data/attention_weights.parquet"));
  const sel = vg.Selection.single();
  return vg.vconcat(
    vg.menu({ from: "attention_weights", column: "run_id", label: "Run", as: sel }),
    vg.plot(
      vg.cell(
        vg.from("attention_weights", { filterBy: sel }),
        {
          x: vg.sql`'Head ' || head`,
          y: vg.sql`'Layer ' || layer`,
          fill: "mean_alpha",
          inset: 0.5
        }
      ),
      vg.width(500),
      vg.height(280),
      vg.colorScheme("blues"),
      vg.colorLegend(true),
      vg.colorLabel("Mean Attention"),
      vg.xLabel("Attention Head"),
      vg.yLabel("GAT Layer")
    )
  );
}
```

### DQN-Fusion Analysis

The learned DQN fusion policy exhibits interpretable, context-specific weighting that validates adaptive expert selection. Analysis reveals a strong correlation between VGAE anomaly scores and fusion weights: low VGAE scores cluster at $\alpha \approx 0$ (favoring the robust expert), while higher scores transition to intermediate weights, demonstrating the policy learned to default to VGAE's out-of-distribution detection while conditionally leveraging GAT's strength on known attacks. The multimodal distribution with peaks at $\alpha \approx 0, 0.2, 0.4, 0.6, 0.8$ indicates the DQN discovered distinct attack-type-specific strategies rather than learning fixed averaging ($\alpha = 0.5$). Critically, the divergence between normal and attack distributions validates meaningful anomaly detection logic.

@fig-dqn-policy shows the learned DQN fusion weight distribution, confirming the multimodal policy structure.

```{ojs}
//| label: fig-dqn-policy
//| fig-cap: "DQN fusion weight (alpha) distribution. The multimodal pattern with peaks at distinct alpha values indicates the policy learned context-specific fusion strategies rather than simple averaging."

{
  await loadTable("dqn_policy", FileAttachment("../data/dqn_policy.parquet"));
  const sel = vg.Selection.single();
  return vg.vconcat(
    vg.menu({ from: "dqn_policy", column: "run_id", label: "Run", as: sel }),
    vg.plot(
      vg.rectY(
        vg.from("dqn_policy", { filterBy: sel }),
        {x: vg.bin("alpha"), y: vg.count(), fill: "steelblue", insetLeft: 0.5, insetRight: 0.5}
      ),
      vg.width(500),
      vg.height(280),
      vg.colorScheme("oranges"),
      vg.xLabel("Alpha (GAT weight in fusion)"),
      vg.yLabel("Count")
    )
  );
}
```

### Layer-wise Knowledge Transfer

To quantify how well the student model replicates the teacher's internal representations, we compute Centered Kernel Alignment (CKA) between corresponding layers. @fig-cka-heatmap shows the layer-wise similarity matrix, where high values along the diagonal indicate successful representation alignment at each depth.

```{ojs}
//| label: fig-cka-heatmap
//| fig-cap: "CKA similarity between teacher and student layers. High diagonal values indicate the student successfully learns representations aligned with the teacher at each network depth."

{
  await loadTable("cka_similarity", FileAttachment("../data/cka_similarity.parquet"));
  const sel = vg.Selection.single();
  return vg.vconcat(
    vg.menu({ from: "cka_similarity", column: "run_id", label: "Run", as: sel }),
    vg.plot(
      vg.cell(
        vg.from("cka_similarity", { filterBy: sel }),
        {
          x: "student_layer",
          y: "teacher_layer",
          fill: "similarity",
          inset: 0.5
        }
      ),
      vg.width(400),
      vg.height(300),
      vg.colorScheme("viridis"),
      vg.colorLegend(true),
      vg.colorLabel("CKA Similarity"),
      vg.xLabel("Student Layer"),
      vg.yLabel("Teacher Layer")
    )
  );
}
```
