# ===========================================================================
# KD-GAT Pipeline Orchestration
#
# Path layout: {EXPERIMENT_ROOT}/{ds}/{model}_{scale}_{stage}[_{aux}]
# All path logic imported from config/paths.py (single source of truth).
# SLURM config imported from config/constants.py (env-overridable).
#
# All datasets:
#   snakemake -s pipeline/Snakefile --profile profiles/slurm --jobs 20
#
# Single dataset:
#   snakemake -s pipeline/Snakefile --config datasets='["hcrl_sa"]' -n
#
# Dry run:
#   snakemake -s pipeline/Snakefile -n
#
# DAG:
#   snakemake -s pipeline/Snakefile --dag | dot -Tpdf > dag.pdf
# ===========================================================================

from config.paths import (
    get_datasets, EXPERIMENT_ROOT,
    checkpoint_path_str, metrics_path_str, benchmark_path_str, log_path_str,
)
from config.constants import SLURM_ACCOUNT, SLURM_PARTITION, SLURM_GPU_TYPE

ALL_DATASETS = get_datasets()
DATASETS     = config.get("datasets", ALL_DATASETS)
import os as _os, sys as _sys
PY        = _os.environ.get("KD_GAT_PYTHON", _sys.executable)
CLI       = f"{PY} -m pipeline.cli"

# Short aliases for rule definitions
# New signatures: (dataset, model_type, scale, stage, aux="")
_ckpt    = checkpoint_path_str
_metrics = metrics_path_str
_bench   = benchmark_path_str
_log     = log_path_str

# Pre-initialize MLflow DB on the login node before any SLURM jobs start.
# Prevents race condition when multiple jobs try to CREATE TABLE concurrently.
onstart:
    shell("{PY} -c 'from pipeline.tracking import setup_tracking; setup_tracking()'")

# After all rules succeed: backup MLflow DB and populate project DB.
onsuccess:
    shell("mkdir -p ~/backups && cp /fs/scratch/PAS1266/kd_gat_mlflow/mlflow.db ~/backups/mlflow_$(date +%Y%m%d).db")
    shell("{PY} -m pipeline.db populate")

# ---------------------------------------------------------------------------
# Targets
# ---------------------------------------------------------------------------

rule all:
    input:
        # Large pipeline
        expand(_ckpt("{ds}", "dqn", "large", "fusion"), ds=DATASETS),
        # Small with KD
        expand(_ckpt("{ds}", "dqn", "small", "fusion", aux="kd"), ds=DATASETS),
        # Small without KD (ablation)
        expand(_ckpt("{ds}", "dqn", "small", "fusion"), ds=DATASETS),

rule large:
    input:
        expand(_ckpt("{ds}", "dqn", "large", "fusion"), ds=DATASETS),

rule small_kd:
    input:
        expand(_ckpt("{ds}", "dqn", "small", "fusion", aux="kd"), ds=DATASETS),

rule small_nokd:
    input:
        expand(_ckpt("{ds}", "dqn", "small", "fusion"), ds=DATASETS),

rule evaluate_all:
    input:
        expand(_metrics("{ds}", "vgae", "large", "evaluation"), ds=DATASETS),
        expand(_metrics("{ds}", "vgae", "small", "evaluation", aux="kd"), ds=DATASETS),
        expand(_metrics("{ds}", "vgae", "small", "evaluation"), ds=DATASETS),

# ---------------------------------------------------------------------------
# SLURM resource defaults
# ---------------------------------------------------------------------------

_SLURM = dict(slurm_account=SLURM_ACCOUNT, slurm_partition=SLURM_PARTITION, gpu_type=SLURM_GPU_TYPE)
_TRAIN_RES = dict(time_min=360, mem_mb=128000, cpus_per_task=16, gpus=1, **_SLURM)
_EVAL_RES = dict(time_min=120, mem_mb=32000, cpus_per_task=8, gpus=1, **_SLURM)

# ===========================================================================
# Large pipeline  (no KD dependencies, runs first)
# ===========================================================================

rule vgae_large:
    output:
        _ckpt("{ds}", "vgae", "large", "autoencoder"),
    benchmark:
        _bench("{ds}", "vgae", "large", "autoencoder")
    resources: **_TRAIN_RES,
    log:
        out=_log("{ds}", "vgae", "large", "autoencoder"),
        err=_log("{ds}", "vgae", "large", "autoencoder", stream="err"),
    shell:
        CLI + " autoencoder --model vgae --scale large --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

rule gat_large:
    input:
        vgae=_ckpt("{ds}", "vgae", "large", "autoencoder"),
    output:
        _ckpt("{ds}", "gat", "large", "curriculum"),
    benchmark:
        _bench("{ds}", "gat", "large", "curriculum")
    resources: **_TRAIN_RES,
    log:
        out=_log("{ds}", "gat", "large", "curriculum"),
        err=_log("{ds}", "gat", "large", "curriculum", stream="err"),
    shell:
        CLI + " curriculum --model gat --scale large --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

rule dqn_large:
    input:
        vgae=_ckpt("{ds}", "vgae", "large", "autoencoder"),
        gat=_ckpt("{ds}", "gat", "large", "curriculum"),
    output:
        _ckpt("{ds}", "dqn", "large", "fusion"),
    benchmark:
        _bench("{ds}", "dqn", "large", "fusion")
    resources: **_TRAIN_RES,
    log:
        out=_log("{ds}", "dqn", "large", "fusion"),
        err=_log("{ds}", "dqn", "large", "fusion", stream="err"),
    shell:
        CLI + " fusion --model dqn --scale large --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

# ===========================================================================
# Small with KD  (each stage depends on large counterpart)
# ===========================================================================

rule vgae_small_kd:
    input:
        teacher=_ckpt("{ds}", "vgae", "large", "autoencoder"),
    output:
        _ckpt("{ds}", "vgae", "small", "autoencoder", aux="kd"),
    benchmark:
        _bench("{ds}", "vgae", "small", "autoencoder", aux="kd")
    resources: **_TRAIN_RES,
    log:
        out=_log("{ds}", "vgae", "small", "autoencoder", aux="kd"),
        err=_log("{ds}", "vgae", "small", "autoencoder", aux="kd", stream="err"),
    shell:
        CLI + " autoencoder --model vgae --scale small --auxiliaries kd_standard"
        " --teacher-path {input.teacher} --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

rule gat_small_kd:
    input:
        teacher=_ckpt("{ds}", "gat", "large", "curriculum"),
        vgae=_ckpt("{ds}", "vgae", "small", "autoencoder", aux="kd"),
    output:
        _ckpt("{ds}", "gat", "small", "curriculum", aux="kd"),
    benchmark:
        _bench("{ds}", "gat", "small", "curriculum", aux="kd")
    resources: **_TRAIN_RES,
    log:
        out=_log("{ds}", "gat", "small", "curriculum", aux="kd"),
        err=_log("{ds}", "gat", "small", "curriculum", aux="kd", stream="err"),
    shell:
        CLI + " curriculum --model gat --scale small --auxiliaries kd_standard"
        " --teacher-path {input.teacher} --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

rule dqn_small_kd:
    input:
        teacher=_ckpt("{ds}", "dqn", "large", "fusion"),
        vgae=_ckpt("{ds}", "vgae", "small", "autoencoder", aux="kd"),
        gat=_ckpt("{ds}", "gat", "small", "curriculum", aux="kd"),
    output:
        _ckpt("{ds}", "dqn", "small", "fusion", aux="kd"),
    benchmark:
        _bench("{ds}", "dqn", "small", "fusion", aux="kd")
    resources: **_TRAIN_RES,
    log:
        out=_log("{ds}", "dqn", "small", "fusion", aux="kd"),
        err=_log("{ds}", "dqn", "small", "fusion", aux="kd", stream="err"),
    shell:
        CLI + " fusion --model dqn --scale small --auxiliaries kd_standard"
        " --teacher-path {input.teacher} --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

# ===========================================================================
# Small without KD  (ablation -- no teacher dependency)
# ===========================================================================

rule vgae_small_nokd:
    output:
        _ckpt("{ds}", "vgae", "small", "autoencoder"),
    benchmark:
        _bench("{ds}", "vgae", "small", "autoencoder")
    resources: **_TRAIN_RES,
    log:
        out=_log("{ds}", "vgae", "small", "autoencoder"),
        err=_log("{ds}", "vgae", "small", "autoencoder", stream="err"),
    shell:
        CLI + " autoencoder --model vgae --scale small --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

rule gat_small_nokd:
    input:
        vgae=_ckpt("{ds}", "vgae", "small", "autoencoder"),
    output:
        _ckpt("{ds}", "gat", "small", "curriculum"),
    benchmark:
        _bench("{ds}", "gat", "small", "curriculum")
    resources: **_TRAIN_RES,
    log:
        out=_log("{ds}", "gat", "small", "curriculum"),
        err=_log("{ds}", "gat", "small", "curriculum", stream="err"),
    shell:
        CLI + " curriculum --model gat --scale small --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

rule dqn_small_nokd:
    input:
        vgae=_ckpt("{ds}", "vgae", "small", "autoencoder"),
        gat=_ckpt("{ds}", "gat", "small", "curriculum"),
    output:
        _ckpt("{ds}", "dqn", "small", "fusion"),
    benchmark:
        _bench("{ds}", "dqn", "small", "fusion")
    resources: **_TRAIN_RES,
    log:
        out=_log("{ds}", "dqn", "small", "fusion"),
        err=_log("{ds}", "dqn", "small", "fusion", stream="err"),
    shell:
        CLI + " fusion --model dqn --scale small --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

# ===========================================================================
# Evaluation  (one rule per pipeline variant)
# ===========================================================================

rule eval_large:
    input:
        vgae=_ckpt("{ds}", "vgae", "large", "autoencoder"),
        gat=_ckpt("{ds}", "gat", "large", "curriculum"),
        dqn=_ckpt("{ds}", "dqn", "large", "fusion"),
    output:
        report(
            _metrics("{ds}", "vgae", "large", "evaluation"),
            category="Evaluation",
            caption="Large model evaluation metrics for dataset {ds}",
        ),
    benchmark:
        _bench("{ds}", "vgae", "large", "evaluation")
    resources: **_EVAL_RES,
    log:
        out=_log("{ds}", "vgae", "large", "evaluation"),
        err=_log("{ds}", "vgae", "large", "evaluation", stream="err"),
    shell:
        CLI + " evaluation --model vgae --scale large --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

rule eval_small_kd:
    input:
        vgae=_ckpt("{ds}", "vgae", "small", "autoencoder", aux="kd"),
        gat=_ckpt("{ds}", "gat", "small", "curriculum", aux="kd"),
        dqn=_ckpt("{ds}", "dqn", "small", "fusion", aux="kd"),
    output:
        report(
            _metrics("{ds}", "vgae", "small", "evaluation", aux="kd"),
            category="Evaluation",
            caption="Small+KD evaluation metrics for dataset {ds}",
        ),
    benchmark:
        _bench("{ds}", "vgae", "small", "evaluation", aux="kd")
    resources: **_EVAL_RES,
    log:
        out=_log("{ds}", "vgae", "small", "evaluation", aux="kd"),
        err=_log("{ds}", "vgae", "small", "evaluation", aux="kd", stream="err"),
    shell:
        CLI + " evaluation --model vgae --scale small --auxiliaries kd_standard --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

rule eval_small_nokd:
    input:
        vgae=_ckpt("{ds}", "vgae", "small", "autoencoder"),
        gat=_ckpt("{ds}", "gat", "small", "curriculum"),
        dqn=_ckpt("{ds}", "dqn", "small", "fusion"),
    output:
        report(
            _metrics("{ds}", "vgae", "small", "evaluation"),
            category="Evaluation",
            caption="Small (no KD) evaluation metrics for dataset {ds}",
        ),
    benchmark:
        _bench("{ds}", "vgae", "small", "evaluation")
    resources: **_EVAL_RES,
    log:
        out=_log("{ds}", "vgae", "small", "evaluation"),
        err=_log("{ds}", "vgae", "small", "evaluation", stream="err"),
    shell:
        CLI + " evaluation --model vgae --scale small --dataset {wildcards.ds}"
        " > {log.out} 2> {log.err}"

# ---------------------------------------------------------------------------
# Reports (parameterized notebooks via Papermill)
# ---------------------------------------------------------------------------

rule notebook_report:
    input:
        _metrics("{ds}", "vgae", "large", "evaluation"),
    output:
        EXPERIMENT_ROOT + "/{ds}/report/analysis.ipynb",
    resources:
        time_min=30, mem_mb=8000, cpus_per_task=4, gpus=0,
        slurm_account=SLURM_ACCOUNT, slurm_partition="serial",
    shell:
        PY + " -m papermill notebooks/03_analytics.ipynb {output} -p dataset {wildcards.ds}"

# ---------------------------------------------------------------------------
# Utilities
# ---------------------------------------------------------------------------

rule clean_logs:
    shell: "find {EXPERIMENT_ROOT} -name 'slurm.*' -delete"

rule clean_all:
    shell: "rm -rf {EXPERIMENT_ROOT}"
