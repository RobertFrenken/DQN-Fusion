model_config:
  type: vgae
  input_dim: 11
  node_embedding_dim: 256
  hidden_dims:
  - 256
  - 128
  - 96
  - 48
  latent_dim: 48
  output_dim: 11
  num_layers: 3
  attention_heads: 8
  dropout: 0.15
  batch_norm: true
  activation: relu
  target_parameters: 1740000
  curriculum_stages:
  - pretrain
  - distill
  hidden_channels: 128
  embedding_dim: 32
  beta: 1.0
training_config:
  mode: autoencoder
  max_epochs: 400
  batch_size: 64
  learning_rate: 0.0005
  weight_decay: 0.0001
  optimizer:
    name: adam
    lr: 0.001
    weight_decay: 0.0001
    momentum: 0.9
  scheduler:
    use_scheduler: false
    scheduler_type: cosine
    params:
      T_max: 100
  early_stopping_patience: 100
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: 32-true
  find_unused_parameters: false
  optimize_batch_size: true
  batch_size_mode: power
  max_batch_size_trials: 10
  run_test: true
  test_every_n_epochs: 5
  save_top_k: 3
  monitor_metric: val_loss
  monitor_mode: min
  log_every_n_steps: 50
  save_hyperparameters: true
  description: Unsupervised autoencoder training on normal samples only
  reconstruction_loss: mse
  use_normal_samples_only: true
model_type: vgae
training_mode: autoencoder
num_ids: 53
