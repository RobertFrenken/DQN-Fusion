# KD-GAT Integration Guide - Complete Walkthrough

This guide walks you through integrating your GitHub repository with the Hydra-Zen system step-by-step.

## Overview of What You're Doing

You're taking your existing KD-GAT code and integrating it with a production-ready Hydra-Zen configuration system. This gives you:

âœ… Type-safe configuration management
âœ… Reproducible experiments with 100+ pre-generated configs
âœ… Deterministic path structure for all results
âœ… MLflow experiment tracking
âœ… Slurm job submission automation
âœ… Zero hardcoded paths in code

## Your Current Setup

Your GitHub repository (lightning branch) has:

```
KD-GAT/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/datasets.py           â† Dataset loading logic
â”‚   â”œâ”€â”€ models/vgae.py             â† VGAE model
â”‚   â”œâ”€â”€ models/gat.py              â† GAT model
â”‚   â”œâ”€â”€ models/dqn.py              â† DQN model (optional)
â”‚   â””â”€â”€ training/trainer.py        â† Your existing trainer
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ automotive/hcrlch/         â† Your actual data
â”‚   â”œâ”€â”€ automotive/set01/
â”‚   â”œâ”€â”€ internet/
â”‚   â””â”€â”€ watertreatment/
â””â”€â”€ requirements.txt
```

## What Gets Added

The Hydra-Zen system adds these files:

```
KD-GAT/
â”œâ”€â”€ hydra_configs/
â”‚   â””â”€â”€ config_store.py            â† NEW: 100+ experiment configs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_with_hydra_zen.py    â† NEW: Main training entry
â”‚   â”‚   â”œâ”€â”€ lightning_modules.py       â† NEW: PyTorch Lightning modules
â”‚   â”‚   â””â”€â”€ trainer.py                 â† Keep your existing trainer
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ experiment_paths.py        â† NEW: Path management
â”œâ”€â”€ oscjobmanager.py               â† NEW: Slurm submission
â””â”€â”€ (other docs)                   â† NEW: Guides and references
```

## Step-by-Step Integration

### Step 1: Copy Core Files (5 minutes)

Copy these files to your project from what you received:

```
hydra_configs/config_store.py       â†’ hydra_configs/config_store.py
src/utils/experiment_paths.py       â†’ src/utils/experiment_paths.py
src/training/train_with_hydra_zen.py â†’ src/training/train_with_hydra_zen.py
src/training/lightning_modules.py   â†’ src/training/lightning_modules.py
oscjobmanager.py                    â†’ oscjobmanager.py
```

Verify files are in place:
```bash
ls -la hydra_configs/config_store.py
ls -la src/utils/experiment_paths.py
ls -la src/training/train_with_hydra_zen.py
ls -la src/training/lightning_modules.py
ls -la oscjobmanager.py
```

### Step 2: Update Dataset Classes (30 minutes)

**File to modify:** `src/data/datasets.py`

**What to do:**
1. Add PyTorch Dataset wrapper class (see INTEGRATION_CODE_TEMPLATES.md Template 1)
2. Update HCRLCHDataset to:
   - Accept `data_path`, `split_ratio`, `normalization` parameters
   - Create `.train`, `.val`, `.test` attributes as Dataset objects
3. Create Set01Dataset, Set02Dataset, Set03Dataset, Set04Dataset classes

**Test it:**
```bash
python3 << 'EOF'
from src.data.datasets import HCRLCHDataset
dataset = HCRLCHDataset('./data/automotive/hcrlch', (0.7, 0.15, 0.15), 'zscore')
print(f"âœ… Train: {len(dataset.train)}, Val: {len(dataset.val)}, Test: {len(dataset.test)}")
EOF
```

### Step 3: Update Model Classes (30 minutes)

**Files to modify:** `src/models/vgae.py`, `src/models/gat.py`, `src/models/dqn.py`

**What to do for EACH model:**
1. Change `__init__` to accept configurable parameters
2. Add `**kwargs` to catch extra arguments
3. Keep all implementation logic exactly the same

**Before:**
```python
def __init__(self):
    super().__init__()
    self.hidden_dim = 64  # Hardcoded!
```

**After:**
```python
def __init__(self, hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.2, **kwargs):
    super().__init__()
    self.hidden_dim = hidden_dim
    self.num_layers = num_layers
    self.dropout = dropout
```

**Test it:**
```bash
python3 -c "from src.models.vgae import VGAE; m = VGAE(hidden_dim=64, latent_dim=32, num_layers=2, dropout=0.1); print('âœ…')"
```

### Step 4: Implement Data Loaders (20 minutes)

**File to modify:** `src/training/train_with_hydra_zen.py`

**What to do:**
1. Find the `load_data_loaders()` function (it's a placeholder)
2. Replace with actual implementation (see INTEGRATION_CODE_TEMPLATES.md Template 3)
3. Import your dataset classes
4. Create DATASET_MAP dictionary
5. Return (train_loader, val_loader, test_loader)

**Test it:**
```bash
python3 << 'EOF'
from src.training.train_with_hydra_zen import load_data_loaders
from omegaconf import OmegaConf

cfg = OmegaConf.create({
    'dataset_config': {
        'name': 'hcrlch',
        'data_path': './data/automotive/hcrlch',
        'split_ratio': (0.7, 0.15, 0.15),
        'normalization': 'zscore',
    },
    'training_config': {'batch_size': 32},
    'num_workers': 0,
    'pin_memory': False,
})

train_loader, val_loader, test_loader = load_data_loaders(cfg)
print(f"âœ… Loaders: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test")
EOF
```

### Step 5: Update Configuration (20 minutes)

**File to modify:** `hydra_configs/config_store.py`

**What to do:**
1. Find `project_root` (around line 30)
2. Update to your actual path:
   ```python
   project_root: str = "/home/username/KD-GAT"  # â† Change this!
   ```
3. Update dataset configs:
   - Verify `data_path` points to existing directories
   - Check: `ls -la ./data/automotive/hcrlch/` works
4. Update model configs:
   - Update `_target_` paths (e.g., "src.models.vgae.VGAE")
   - Update parameters to match your models

**Test it:**
```bash
python src/training/train_with_hydra_zen.py \
    config_store=automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    --cfg job | head -30
```

Expected: Config prints with your paths

### Step 6: Link Models to Lightning (20 minutes)

**File to modify:** `src/training/lightning_modules.py`

**What to do:**
1. Find `_build_vgae()` method
2. Import your actual VGAE class
3. Instantiate with config parameters
4. Repeat for `_build_gat()` and `_build_dqn()`

**Example for VGAE:**
```python
def _build_vgae(self) -> nn.Module:
    from src.models.vgae import VGAE
    
    return VGAE(
        input_dim=self.cfg.model_config.input_dim,
        hidden_dim=self.cfg.model_size_config.hidden_dim,
        latent_dim=self.cfg.model_config.latent_dim,
        num_layers=self.cfg.model_size_config.num_layers,
        dropout=self.cfg.model_size_config.dropout,
    )
```

**Test it:**
```bash
python src/training/train_with_hydra_zen.py \
    config_store=automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    --cfg job | grep "model_architecture:"
```

### Step 7: Run Single-Epoch Training (5-10 minutes)

**First test on CPU to verify everything works:**

```bash
python src/training/train_with_hydra_zen.py \
    config_store=automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    device=cpu \
    training_config.epochs=1
```

**Expected output:**
- Data loads successfully
- Model builds successfully
- Training runs 1 epoch
- Results save to: `experimentruns/automotive/hcrlch/unsupervised/VGAE/student/no/all_samples/run_000/`

**Verify results:**
```bash
ls -la experimentruns/automotive/hcrlch/unsupervised/VGAE/student/no/all_samples/run_000/
# Should have: model.pt, config.yaml, checkpoints/, training_metrics.json
```

### Step 8: Run Hyperparameter Sweep (optional)

```bash
python src/training/train_with_hydra_zen.py -m \
    config_store=automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    model_size_config.hidden_dim=32,64,128,256 \
    device=cpu \
    training_config.epochs=1
```

Creates multiple runs: run_000, run_001, run_002, run_003

### Step 9: Submit to Slurm (on OSC)

```bash
# Preview the script
python oscjobmanager.py submit \
    automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    --dry-run

# Actually submit
python oscjobmanager.py submit \
    automotive_hcrlch_unsupervised_vgae_student_no_all_samples
```

## File Organization After Integration

Your final project structure:

```
KD-GAT/
â”œâ”€â”€ hydra_configs/
â”‚   â””â”€â”€ config_store.py                    â† 100+ pre-generated configs
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ datasets.py                    â† Your dataset classes (updated)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ vgae.py                        â† Your VGAE (updated)
â”‚   â”‚   â”œâ”€â”€ gat.py                         â† Your GAT (updated)
â”‚   â”‚   â””â”€â”€ dqn.py                         â† Your DQN (updated)
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_with_hydra_zen.py        â† Main entry point (new)
â”‚   â”‚   â”œâ”€â”€ lightning_modules.py           â† Lightning modules (new)
â”‚   â”‚   â””â”€â”€ trainer.py                     â† Keep your original
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ experiment_paths.py            â† Path management (new)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ automotive/
â”‚   â”‚   â”œâ”€â”€ hcrlch/                        â† Your data (unchanged)
â”‚   â”‚   â”œâ”€â”€ set01/
â”‚   â”‚   â”œâ”€â”€ set02/
â”‚   â”‚   â”œâ”€â”€ set03/
â”‚   â”‚   â””â”€â”€ set04/
â”‚   â”œâ”€â”€ internet/
â”‚   â””â”€â”€ watertreatment/
â”‚
â”œâ”€â”€ experimentruns/                        â† Results directory (created on first run)
â”‚   â””â”€â”€ automotive/
â”‚       â””â”€â”€ hcrlch/
â”‚           â””â”€â”€ unsupervised/
â”‚               â””â”€â”€ VGAE/
â”‚                   â””â”€â”€ student/
â”‚                       â””â”€â”€ no/
â”‚                           â””â”€â”€ all_samples/
â”‚                               â”œâ”€â”€ run_000/
â”‚                               â”œâ”€â”€ run_001/
â”‚                               â””â”€â”€ ...
â”‚
â”œâ”€â”€ oscjobmanager.py                       â† Slurm job manager (new)
â”œâ”€â”€ requirements.txt                       â† Keep your requirements
â””â”€â”€ (documentation files)
```

## Quick Reference: Common Commands

**Single experiment (CPU):**
```bash
python src/training/train_with_hydra_zen.py \
    config_store=automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    device=cpu \
    training_config.epochs=5
```

**Single experiment (GPU):**
```bash
python src/training/train_with_hydra_zen.py \
    config_store=automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    device=cuda \
    training_config.epochs=100
```

**Hyperparameter sweep:**
```bash
python src/training/train_with_hydra_zen.py -m \
    config_store=automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    model_size_config.hidden_dim=32,64,128
```

**Different config:**
```bash
python src/training/train_with_hydra_zen.py \
    config_store=automotive_set01_classifier_gat_teacher_standard_all_samples
```

**View config before running:**
```bash
python src/training/train_with_hydra_zen.py \
    config_store=automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    --cfg job
```

**Submit to Slurm:**
```bash
python oscjobmanager.py submit automotive_hcrlch_unsupervised_vgae_student_no_all_samples
```

**Preview Slurm script:**
```bash
python oscjobmanager.py submit \
    automotive_hcrlch_unsupervised_vgae_student_no_all_samples \
    --dry-run
```

## Troubleshooting

**If something breaks:**

1. Check error message carefully (usually tells you what's wrong)
2. See `INTEGRATION_DEBUGGING.md` for common issues and solutions
3. Verify paths: `ls -la src/data/datasets.py` etc.
4. Test individual components:
   ```bash
   # Test imports
   python3 -c "from src.data.datasets import HCRLCHDataset; print('âœ…')"
   
   # Test config
   python src/training/train_with_hydra_zen.py config_store=name --cfg job
   
   # Test training
   python src/training/train_with_hydra_zen.py config_store=name device=cpu training_config.epochs=1
   ```

## Key Concepts

**Config Hierarchy (8 levels):**
```
modality (automotive, internet, watertreatment)
  â†’ dataset (hcrlch, set01, set02, set03, set04)
    â†’ learning_type (unsupervised, classifier, fusion)
      â†’ model_architecture (VGAE, GAT, DQN)
        â†’ model_size (teacher, student, intermediate, huge, tiny)
          â†’ distillation (no, standard, topology_preserving)
            â†’ training_mode (all_samples, normals_only, curriculum_*)
              â†’ run_000, run_001, ... (auto-incremented)
```

**Config Name Format:**
```
{modality}_{dataset}_{learning_type}_{model_arch}_{model_size}_{distillation}_{training_mode}
```

**Example:**
```
automotive_hcrlch_unsupervised_vgae_student_no_all_samples
automotive_hcrlch_classifier_gat_teacher_standard_curriculum_classifier
internet_set02_fusion_dqn_intermediate_topology_preserving_all_samples
```

## Success Criteria

You're done when you can:

- [ ] Run: `python src/training/train_with_hydra_zen.py config_store=automotive_hcrlch_unsupervised_vgae_student_no_all_samples device=cpu training_config.epochs=1`
- [ ] See results save to: `experimentruns/automotive/hcrlch/.../run_000/`
- [ ] Config file saved: `experimentruns/.../run_000/config.yaml`
- [ ] Model saved: `experimentruns/.../run_000/model.pt`
- [ ] Run sweep: `python src/training/train_with_hydra_zen.py -m config_store=name model_size_config.hidden_dim=32,64,128`
- [ ] Submit to Slurm: `python oscjobmanager.py submit name` (on OSC)

## Documentation Files

| File | Purpose |
|------|---------|
| **README_INTEGRATION.md** | Quick start (you read this first) |
| **INTEGRATION_SUMMARY.md** | 5-step overview |
| **INTEGRATION_TODO.md** | Checklist to track progress |
| **INTEGRATION_CODE_TEMPLATES.md** | Copy-paste code |
| **INTEGRATION_DEBUGGING.md** | Fixing common errors |
| **ARCHITECTURE_SUMMARY.md** | System design |
| **QUICK_REFERENCE.md** | Command cheat sheet |
| **SETUP_CHECKLIST.md** | Verification steps |

## Timeline

- Reading docs: 1 hour
- Making code changes: 2 hours
- Testing: 1-2 hours
- **Total: 4-5 hours**

Ready to start? Open `INTEGRATION_TODO.md` and check off items as you go! ğŸš€
