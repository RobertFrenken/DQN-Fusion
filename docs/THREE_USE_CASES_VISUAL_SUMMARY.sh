#!/usr/bin/bash
# Three Use Cases Visual Guide

cat << 'EOF'

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                           â•‘
â•‘               HOW YOUR PROJECT WORKS: THREE USE CASES                    â•‘
â•‘                                                                           â•‘
â•‘                 Individual | Distillation | Fusion                       â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ï¸âƒ£  INDIVIDUAL TRAINING                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Purpose: Train a single model independently                           â”‚
â”‚  Use Cases: Baseline performance, single-model inference               â”‚
â”‚                                                                         â”‚
â”‚  Two Variants:                                                          â”‚
â”‚                                                                         â”‚
â”‚  A) GAT Classifier (Supervised)                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚  Raw CAN Data                                                           â”‚
â”‚      â†“                                                                  â”‚
â”‚  [Load Dataset] â†’ Create graphs                                        â”‚
â”‚      â†“                                                                  â”‚
â”‚  [GATWithJK Model]                                                      â”‚
â”‚  (Graph Attention Networks with Jumping Knowledge)                     â”‚
â”‚      â†“                                                                  â”‚
â”‚  Output: Classification logits (2 classes: normal/attack)             â”‚
â”‚      â†“                                                                  â”‚
â”‚  Loss: CrossEntropy(predictions, ground_truth_labels)                â”‚
â”‚      â†“                                                                  â”‚
â”‚  Result: Supervised classifier trained                                 â”‚
â”‚  Accuracy: ~96%                                                         â”‚
â”‚                                                                         â”‚
â”‚  B) VGAE Autoencoder (Unsupervised)                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚  Raw CAN Data (ONLY NORMAL SAMPLES!)                                   â”‚
â”‚      â†“                                                                  â”‚
â”‚  [Load Dataset] â†’ Filter to label==0                                   â”‚
â”‚      â†“                                                                  â”‚
â”‚  [GraphAutoencoderNeighborhood - VGAE]                                 â”‚
â”‚  (Variational Graph AutoEncoder)                                       â”‚
â”‚      â†“                                                                  â”‚
â”‚  Outputs:                                                               â”‚
â”‚    â€¢ Reconstructed features (continuous values)                        â”‚
â”‚    â€¢ CAN ID predictions                                                â”‚
â”‚    â€¢ Latent representation z                                           â”‚
â”‚    â€¢ KL divergence (variational term)                                  â”‚
â”‚      â†“                                                                  â”‚
â”‚  Loss: Reconstruction + CAN_ID + 0.01Â·KL                              â”‚
â”‚      â†“                                                                  â”‚
â”‚  Result: Autoencoder learns "normal" distribution                      â”‚
â”‚  Anomaly Detection: reconstruction_error > threshold â†’ ATTACK          â”‚
â”‚  Accuracy: ~92%                                                         â”‚
â”‚                                                                         â”‚
â”‚  Command:                                                               â”‚
â”‚  $ python train_with_hydra_zen.py --model gat --training normal        â”‚
â”‚  $ python train_with_hydra_zen.py --training autoencoder               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ï¸âƒ£  KNOWLEDGE DISTILLATION                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Purpose: Compress large teacher into small student                    â”‚
â”‚  Benefits: 2-4x faster inference, 75% fewer parameters                 â”‚
â”‚                                                                         â”‚
â”‚  Architecture:                                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚                                                                         â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚             â”‚ Training Input (Raw CAN Data)       â”‚                    â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                            â”‚                                            â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚           â†“                                  â†“                          â”‚
â”‚  [Teacher Model]                    [Student Model]                    â”‚
â”‚  (Large, pre-trained)              (Small, to train)                   â”‚
â”‚  (Frozen, no gradient)             (Learning)                          â”‚
â”‚           â”‚                                  â”‚                          â”‚
â”‚           â†“                                  â†“                          â”‚
â”‚  [Teacher Output]                  [Student Output]                    â”‚
â”‚  (at temperature T=4.0)            (at temperature T=4.0)              â”‚
â”‚           â”‚                                  â”‚                          â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                            â†“                                            â”‚
â”‚                    [KL Divergence Loss]                                â”‚
â”‚            (soft targets at T=4.0, scaled by TÂ²)                       â”‚
â”‚                            â”‚                                            â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚           â†“                                  â†“                          â”‚
â”‚     [Soft Loss]                      [Hard Loss]                       â”‚
â”‚  (KD distillation)               (Task loss on labels)                 â”‚
â”‚           â”‚                                  â”‚                          â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                            â†“                                            â”‚
â”‚             Total Loss = 0.7Â·soft + 0.3Â·hard                           â”‚
â”‚                            â†“                                            â”‚
â”‚                  Backprop through student only                         â”‚
â”‚                            â†“                                            â”‚
â”‚           Smaller, faster student with high accuracy                   â”‚
â”‚                                                                         â”‚
â”‚  Temperature Parameter Effect:                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚                                                                         â”‚
â”‚  Without Temperature (T=1):                                             â”‚
â”‚    Teacher output: [0.99, 0.01] (very confident)                      â”‚
â”‚    Student learns: "Class 1 is correct" (little signal for class 2)   â”‚
â”‚                                                                         â”‚
â”‚  With Temperature (T=4):                                                â”‚
â”‚    Teacher output: [0.73, 0.27] (softened)                            â”‚
â”‚    Student learns: "Class 1 is likely, but class 2 is possible"       â”‚
â”‚    Much more learning signal!                                          â”‚
â”‚                                                                         â”‚
â”‚  Command:                                                               â”‚
â”‚  $ python train_with_hydra_zen.py --training knowledge_distillation \\â”‚
â”‚      --teacher_path saved_models/best_teacher_model_hcrl_sa.pth \\   â”‚
â”‚      --student_scale 0.5                                               â”‚
â”‚                                                                         â”‚
â”‚  Results:                                                               â”‚
â”‚    Teacher accuracy: 96%                                                â”‚
â”‚    Student accuracy: 95% (only 1% loss)                                â”‚
â”‚    Student size: 25% of teacher                                        â”‚
â”‚    Inference speed: 3-4x faster                                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ï¸âƒ£  FUSION TRAINING WITH DQN                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Purpose: Learn optimal weighting of VGAE + GAT                        â”‚
â”‚  Benefit: 2-5% accuracy improvement, combine strengths of both         â”‚
â”‚                                                                         â”‚
â”‚  Architecture:                                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚                                                                         â”‚
â”‚  Step 1: Prediction Caching (One-time, 2-5 minutes)                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚                                                                         â”‚
â”‚    Pre-trained VGAE    Pre-trained GAT                                 â”‚
â”‚          â”‚                    â”‚                                         â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                   â†“                                                     â”‚
â”‚  Run all data through both models                                      â”‚
â”‚                   â†“                                                     â”‚
â”‚    Save VGAE anomaly_scores.pkl                                        â”‚
â”‚    Save GAT gat_probs.pkl                                              â”‚
â”‚                   â†“                                                     â”‚
â”‚  (No more forward passes needed for training!)                         â”‚
â”‚                                                                         â”‚
â”‚  Step 2: DQN Training (3-10 minutes for 50 epochs)                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚                                                                         â”‚
â”‚    [Load cached predictions]                                           â”‚
â”‚           â†“                                                             â”‚
â”‚    For each sample:                                                     â”‚
â”‚      anomaly_score âˆˆ [0, 1]   (from VGAE)                             â”‚
â”‚      gat_prob âˆˆ [0, 1]        (from GAT)                              â”‚
â”‚           â†“                                                             â”‚
â”‚    [Discretize to state]                                               â”‚
â”‚      state = (bin(anomaly_score), bin(gat_prob))                       â”‚
â”‚      state âˆˆ {0-10} Ã— {0-10}   (100 possible states)                   â”‚
â”‚           â†“                                                             â”‚
â”‚    [Q-Network (DQN)]                                                    â”‚
â”‚      input: state (discretized prediction pair)                        â”‚
â”‚      output: Q-values for 21 actions (Î± = 0.0 to 1.0)                 â”‚
â”‚           â†“                                                             â”‚
â”‚    [Epsilon-Greedy Action Selection]                                   â”‚
â”‚      if random() < epsilon (exploration):                              â”‚
â”‚          action = random_action()                                       â”‚
â”‚      else:                                                              â”‚
â”‚          action = argmax(Q-values)  (exploitation)                     â”‚
â”‚           â†“                                                             â”‚
â”‚    [Compute Fusion Weight]                                             â”‚
â”‚      alpha = action / (num_actions - 1)                                â”‚
â”‚      alpha âˆˆ [0, 1]                                                    â”‚
â”‚           â†“                                                             â”‚
â”‚    [Fuse Predictions]                                                  â”‚
â”‚      fused_score = alphaÂ·gat_prob + (1-alpha)Â·anomaly_score           â”‚
â”‚           â†“                                                             â”‚
â”‚    [Compute Reward]                                                    â”‚
â”‚      if fused_score > threshold and label == attack: reward = 1       â”‚
â”‚      else: reward = 0                                                  â”‚
â”‚           â†“                                                             â”‚
â”‚    [Experience Replay]                                                 â”‚
â”‚      Store: (state, action, reward, next_state, done)                 â”‚
â”‚           â†“                                                             â”‚
â”‚    [Q-Learning Update]                                                 â”‚
â”‚      Sample minibatch from replay buffer                               â”‚
â”‚      Q-target = reward + gammaÂ·max(Q(next_state))                      â”‚
â”‚      Q-pred = Q(state)[action]                                         â”‚
â”‚      loss = MSE(Q-pred, Q-target)                                      â”‚
â”‚           â†“                                                             â”‚
â”‚    [Backprop DQN]                                                      â”‚
â”‚      Update Q-network weights                                          â”‚
â”‚           â†“                                                             â”‚
â”‚    [Target Network Update]                                             â”‚
â”‚      Every 100 steps: copy Q-network to target Q-network               â”‚
â”‚           â†“                                                             â”‚
â”‚    [Decay Exploration]                                                 â”‚
â”‚      epsilon *= 0.995 (gradually trust learned policy)                 â”‚
â”‚           â†“                                                             â”‚
â”‚    Repeat for all training samples                                      â”‚
â”‚                                                                         â”‚
â”‚  Step 3: Learned Policy Heatmap                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚                                                                         â”‚
â”‚    The DQN learns: for each (anomaly_score, gat_prob) pair,           â”‚
â”‚    what weight Î± should we use?                                        â”‚
â”‚                                                                         â”‚
â”‚    Heatmap visualization:                                              â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚      VGAE Anomaly Score â†’                                              â”‚
â”‚      0.0       0.5       1.0                                           â”‚
â”‚  1.0 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚      â”‚ blue  purple   red     â”‚  â† GAT Attack Prob                    â”‚
â”‚      â”‚ Î±=0.1  Î±=0.5  Î±=0.9    â”‚                                        â”‚
â”‚  0.5 â”‚ blue  purple   red     â”‚                                        â”‚
â”‚      â”‚ Î±=0.2  Î±=0.5  Î±=0.8    â”‚                                        â”‚
â”‚  0.0 â”‚ blue  purple   red     â”‚                                        â”‚
â”‚      â”‚ Î±=0.3  Î±=0.5  Î±=0.7    â”‚                                        â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                                                                         â”‚
â”‚    Blue = Low Î± (trust VGAE)                                           â”‚
â”‚    Red = High Î± (trust GAT)                                            â”‚
â”‚    Purple = Medium Î± (balanced)                                        â”‚
â”‚                                                                         â”‚
â”‚  Command:                                                               â”‚
â”‚  $ python train_fusion_lightning.py --dataset hcrl_sa                  â”‚
â”‚                                                                         â”‚
â”‚  Results:                                                               â”‚
â”‚    VGAE alone: 92%                                                     â”‚
â”‚    GAT alone: 96%                                                      â”‚
â”‚    Fusion (learned): 98%                                               â”‚
â”‚    Improvement: +2% over best single model                             â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š SIDE-BY-SIDE COMPARISON                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚                    Individual    Distillation    Fusion                â”‚
â”‚                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€                â”‚
â”‚ Training Time       10-30 min      15-40 min     6-15 min             â”‚
â”‚ Models Used         1              2 (1 frozen)  3 (2 frozen)         â”‚
â”‚ Complexity          Low            Medium        High                 â”‚
â”‚ Inference Speed     Fast           Fastest       Medium               â”‚
â”‚ Accuracy (VGAE)     92%            ~90%          Not used             â”‚
â”‚ Accuracy (GAT)      96%            96%           Not used             â”‚
â”‚ Final Accuracy      96%            95%           98%                  â”‚
â”‚ Best For            Baseline       Edge devices  Best accuracy        â”‚
â”‚ Memory (inference)  Baseline       1/3 - 1/2     2x baseline          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”„ TYPICAL WORKFLOW                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Step 1: Try Individual Models                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  $ python train_with_hydra_zen.py --model gat --training normal        â”‚
â”‚  $ python train_with_hydra_zen.py --training autoencoder               â”‚
â”‚  Result: VGAE=92%, GAT=96%                                             â”‚
â”‚                                                                         â”‚
â”‚  Step 2 (optional): Compress for Deployment                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  $ python train_with_hydra_zen.py --training knowledge_distillation    â”‚
â”‚  Result: Student=95% accuracy, 3x faster                               â”‚
â”‚                                                                         â”‚
â”‚  Step 3: Maximize Accuracy                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  $ python train_fusion_lightning.py --dataset hcrl_sa                  â”‚
â”‚  Result: Fusion=98% accuracy (best of both worlds!)                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“š KEY CONCEPTS                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚ Individual:                                                             â”‚
â”‚   GAT = Graph Attention Network (supervised learning)                   â”‚
â”‚   VGAE = Variational Graph AutoEncoder (unsupervised learning)         â”‚
â”‚   Different strengths: GAT learns decision boundary, VGAE learns dist  â”‚
â”‚                                                                         â”‚
â”‚ Distillation:                                                           â”‚
â”‚   Temperature = softening parameter (helps with knowledge transfer)    â”‚
â”‚   Hard loss = task loss (on labels)                                    â”‚
â”‚   Soft loss = KL divergence (matches teacher logits)                   â”‚
â”‚   Student learns HOW teacher reasons, not just WHAT it outputs        â”‚
â”‚                                                                         â”‚
â”‚ Fusion:                                                                 â”‚
â”‚   Q-Network = neural network that learns Q(state, action) values      â”‚
â”‚   Experience Replay = remember past decisions, learn from them        â”‚
â”‚   Target Network = separate copy for stability                         â”‚
â”‚   Epsilon-Greedy = balance exploration (random) vs exploitation (best)â”‚
â”‚   DQN learns: Î±(anomaly_score, gat_prob) â†’ optimal fusion weight      â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EOF

echo ""
echo "For detailed explanation, see: THREE_USE_CASES_EXPLAINED.md"
echo ""
